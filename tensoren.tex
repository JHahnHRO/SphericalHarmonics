% !TeX root = spherical_harmonics.tex
% !TeX spellcheck = de_DE

\subsection{Wiederholung: Vektorräume}
\input{vektorraeume.tex}

\subsection{Bilineare Abbildungen}

\begin{remark}
Es gibt i.A. keine Multiplikation zweier Vektoren miteinander in irgendeinem Sinne. Wir können immer Skalare mit Vektoren multiplizieren, aber nicht Vektoren mit Vektoren. Nichts desto trotz ist es \emph{manchmal} doch so, dass zusätzlich zu Addition und Skalarmultiplikation eine weitere Operation existiert, die ein sinnvolles Konzept von Multiplikation liefert, z.B.
\begin{enumerate}
	\item Der Vektorraum der Polynome $\IR[X]$ hat die Polynommultiplikation, d.h.
    \[\left(\sum_{i=0}^n a_i X^i\right) \cdot \left(\sum_{j=0}^m b_j X^j\right) := \sum_{k=0}^{n+m} (\sum_{\substack{i,j \\ i+j=k}} a_i b_j) X^k\]
	\item Der Vektorraum der Funktionen $X\to\IC$ für einen festen Definitionsbereich $X$ hat die punktweise Multiplikation, d.h.
    \[f\cdot g := x\mapsto f(x)g(x)\]
	\item Die Hintereinanderausführung von linearen Abbildungen $\Hom_K(V,W) \times \Hom_K(U,V) \to \Hom_K(U,W), (f,g) \mapsto f\circ g$ ist eine Abbildung, die sich in vielerlei Hinsicht auch wie eine Multiplikation verhält. Für $U=V=W$ erhält man insbesondere eine Multiplikation $\End_K(V) \times \End_K(V) \to \End_K(V)$.
    
    Nach Wahl von je einer Basis können wir $\Hom_K(V,W)$, $\Hom_K(U,V)$ und $\Hom_K(U,W)$ mit $K^{n\times m}$, $K^{m\times p}$ und $K^{n\times p}$ identifizieren. Die Hintereinanderausführung von linearen Abbildung entspricht dann der Matrixmultiplikation.
    \item ...
\end{enumerate}

\end{remark}

\begin{definition}[Bilineare \& multilineare Abbildungen]\label{bilineare_abb:def}
Sind $V,W,X$ drei $K$-Vektorräume, so heißt eine Abbildung $\phi: V\times W\to X$ \emph{bilinear}, falls sie beiden Linearitätsbedingungen in Tabelle \ref{bilineare_abb:def_table} erfüllt, d.h. die Funktion ist separat linear, wenn man nur den ersten oder nur den zweiten Input variiert und den anderen festhält.

\begin{table}[!ht]
    \setlength\extrarowheight{10pt} % for a bit of visual "breathing space"
    \hspace{-0.1\textwidth}
    \begin{tabularx}{1.2\textwidth}{p{6.5cm} p{9.5cm}}
        \toprule
        \textbf{Axiom}                  & \textbf{Bedeutung} \hspace{0.5cm} \\ 
        \midrule
        Linearität: \\
        \hspace{1cm}im ersten Argument  & $\forall v_1,v_2\in V, w\in W: \phi(v_1+v_2,w) = \phi(v_1,w) + \phi(v_2,w)$ \\
                                        & $\forall \lambda\in K, v\in W, w\in W: \phi(\lambda v, w) = \lambda \phi(v,w)$ \\
        \hspace{1cm}im zweiten Argument & $\forall v\in V, w_1, w_2\in W: \phi(v,w_1+w_2) = \phi(v,w_1) + \phi(v,w_2)$ \\
                                        & $\forall \lambda\in K, v\in W, w\in W: \phi(v, \lambda w) = \lambda \phi(v,w)$ \\
        Falls $\phi: V\times V \to X$, kann optional gelten: \\
        \hspace{1cm}Symmetrie bzw. Kommutativität & $\forall u,v\in V: \phi(u,v) = \phi(v,u)$ \\
        \hspace{1cm}Antisymmetrie & $\forall u,v\in V: \phi(u,v) = -\phi(v,u)$ \\
        Falls $\phi: V\times V\to V$, kann optional gelten: \\
        \hspace{1cm}Assoziativität & $\forall u,v,w\in V: \phi(u,\phi(v,w)) = \phi(\phi(u,v),w)$ \\
        \bottomrule
    \end{tabularx}
    \label{bilineare_abb:def_table}
    \caption{Eigenschaften von bilinearen Abbildungen}
\end{table}

Für Abbildungen $V_1\times V_2\times V_3\to X$, die von drei Inputvektoren abhängig sind, kann man entsprechend definieren, dass eine Abbildung \emph{trilinear} heißt, wenn sie die drei Distributivgesetze erfüllt. Für vier, fünf, ... $k$ Input-Vektoren spricht man entsprechend von \emph{$k$-fach linearen} Abbildungen.
\end{definition}

\begin{example}
Die drei genannten \enquote{Multiplikationen} von oben sind bilinear. Polynommultiplikation und punktweise Multiplikation von Funktionen sind kommutativ und assoziativ. Die Komposition $\End_K(V)\times\End_K(V)\to\End_K(V)$ ist assoziativ, aber nicht kommutativ, falls $\dim(V) > 1$.

Es gibt viele weitere, äußerst nützliche Beispiele.
\begin{enumerate}[start=4]
	\item Richtungsableitungen sind bilinear in der Richtung und der Funktion:
	
	 Sei $X\subseteq\IR^n$ ein geeigneter Definitionsbereich (z.B. eine offene Menge). Sei außerdem $f:X \to\IC$ eine differenzierbare Funktion. Dann existieren insbesondere alle Richtungsableitungen $(\partial_vf)(x_0) = \lim_{t\to 0} \frac{f(x_0)-f(x_0+tv)}{t}$ in allen Punkten $x_0\in X$. Die Abbildungsvorschrift $(v,f) \mapsto \partial_v f$ liefert eine bilineare Abbildung für mehrere Kombinationen von Vektorräumen, z.B. $\IR^n \times C^1(\IR^n) \to C^0(\IR^n)$ oder $\IR^n \times C^\infty(\IR^n) \to C^\infty(\IR^n)$.
     
     \item Allgemeiner: Lineare Differentialoperatoren:
     
     Für mehrfach differenzierbare Funktionen kann man natürlich auch mehrere Ableitungsschritte hintereinander ausführen. Auf diese Weise erhält man multilineare Abbildungen: $k$-faches Ableiten in $k$ Richtungen, also der Differentialoperator $\partial_{v_1} \partial_{v_2} \cdots \partial_{v_k}$ ist $k$-fach linear in den Vektoren $v_1, ..., v_k$ als Inputs. Die Anwendung auf eine Funktion ist entsprechend $(k+1)$-fach linear in den $k$ Richtungsvektoren und der Funktion als Inputs.
     
     Als $k$-lineare Abbildung ist dies eine symmetrische Abbildung, d.h. $\partial_v \partial w f = \partial_w\partial_v f$, sofern $f$ zweimal stetig differenzierbar ist. Das ist der Satz von Schwarz\footnote{Hermann Amandus Schwarz (1843 -- 1921), dt. Mathematiker.}.
     
     \item Kreuzprodukt:
     
     Es gibt eine besondere bilineare Abbildung $\textsc{x} : \IR^3 \times \IR^3 \to \IR^3$, genannt \udot{Kreuzprodukt}, die in anderen Dimensionen keine gute Analogie hat, nämlich
     
     \[\begin{pmatrix}x_1\\y_1\\z_1\end{pmatrix} \textsc{x} \begin{pmatrix}x_2\\y_2\\z_2\end{pmatrix} := \begin{pmatrix}
     y_1 z_2 - y_2 z_1 \\
     x_1 z_2 - x_2 z_1 \\
     x_1 y_2 - x_2 y_1
     \end{pmatrix}\]
     Es ist bilinear, aber weder kommutativ noch assoziativ. Stattdessen ist es antisymmetrisch.
\end{enumerate}
\end{example}

\subsection{Das Tensorprodukt}

\begin{remark}
Bilineare Abbildungen sind in vielerlei Hinsicht ähnlich zu linearen Abbildungen, z.B. kann man sie durch Basiswahl eindeutig beschreiben, d.h.

\medbreak
Wenn $V,W$ zwei $K$-Vektorräume sind, $b_1,\ldots,b_n$ eine Basis von $V$ und $c_1, \ldots c_m$ eine Basis von $W$, und $\phi: V\times W\to X$ eine bilineare Abbildung, dann sind alle Werte $\phi(v,w)$ bereits eindeutig festgelegt, wenn man $\phi(b_i,c_j)$ für alle $i$ und $j$ kennt, und umgekehrt liefert jede beliebige Wahl von Vektoren $x_{ij}\in X$ genau eine bilineare Abbildung $\psi: V\times W\to X$, die $\psi(b_i,c_j) = x_{ij}$ erfüllt.

\medbreak
Weiter könnte man z.B. jetzt den Raum aller bilinearen Abbildungen $Bil(V,W;X)$ betrachten und beweisen, dass das selbst ein Vektorraum ist genau wie der Raum der linearen Abbildungen zwischen zwei festen Vektorräumen selbst ein Vektorraum ist.

\medbreak
Ähnliches gilt auch für $k$-lineare Abbildungen. Außerdem kann man sich davon überzeugen, dass diese Strukturen mit Hintereinanderausführung von Abbildungen verträglich sind. Hier ist noch zu beachten, dass multilineare Abbildungen viel mehr Möglichkeiten haben, zwei Abbildungen hintereinander auszuführen, z.B. könnte man zwei bilineare Abbildungen hintereinander ausführen, indem man
\[\phi(\psi(u,v),w)\quad\text{oder}\quad \phi(u,\psi(v,w))\]
bildet und i.A. sind das zwei verschiedene $3$-lineare Abbildungen. Man hat bereits drei fundamental verschiedene Möglichkeiten, lineare und bilineare Abbildungen miteinander zu kombinieren: Man kann $\phi(\alpha(u),v)$, $\phi(u,\alpha(w))$ oder $\alpha(\phi(u,v))$ bilden je nachdem, was davon tatsächlich definiert ist. Alle Kombinationen sind jedoch wieder bilineare Abbildungen. Wenn man allgemein $k$- und $m$-lineare Abbildungen miteinander kombinieren will, hat man sehr schnell eine explodierende Anzahl verschiedener Möglichkeiten vor sich. Sie alle liefern wieder multilineare Abbildungen als Ergebnis und alle Möglichkeiten $a$ verschiedene multilineare Abbildungen auf diese Weise zu kombinieren, sind selbst $a$-fach lineare Operationen zwischen den entsprechenden Abbildungsräumen.

\medbreak
Man könnte all das jetzt für alle diese Kombinationen beweisen, wenn man zu viel Freizeit und Tinte hat. Alle Beweise sind langweilig, wenn man den linearen Fall einmal verstanden hat und funktionieren in der Tat völlig analog.\footnote{Wer's nicht glaubt, probiere es selbst aus und finde alle diese Beweise, bis das notwendige Maß an Langeweile erreicht ist.} Das große Problem ist eigentlich, eine geeignete Notation zu finden, die einem erlaubt, diese Beweise alle nur einmal und dafür allgemein zu führen, statt in jeder neuen Kombination von vorne anfangen zu müssen. Und selbst wenn man sich so eine Notation überlegt, dann steht man noch vor einigen technischen, aber völlig trivialen Problemen, die mehr Arbeit verlangen als man für solche Trivialitäten erwartet. Wenn man beispielsweise eine Notation erfindet um zwei multilineare Abbildungen miteinander zu verbinden, dann stellt sich die Frage, ob alle Arten, drei multilineare Abbildungen zu verbinden, sich durch schrittweises Verbinden von je zweien erhalten lassen und ob dafür eine geeignete Form von Assoziativität gilt. Erneut stellt man fest, dass die Antwort \enquote{ja} ist, die geeignete Form von Assoziativität gilt und die Beweise alle analog zum linearen Fall laufen.
\end{remark}

\begin{remark}
All diese Fälle sind letztendlich so sehr ähnlich zum linearen Fall, dass es in der Tat eine Konstruktion gibt, die es einem erlaubt, multilineare Abbildungen als echte lineare Abbildungen aufzufassen, sodass man überhaupt nichts mehr beweisen muss, das über den (schon bekannten) linearen Fall und die Existenz und grundlegenden Eigenschaften dieser Konstruktion hinaus geht. Dieses Konstrukt ist das \emph{Tensorprodukt}.
\end{remark}

\begin{theoremdef}[Universelle Eigenschaft und Existenz des Tensorprodukts]\label{tensorprodukt:def}
Es seien $V,W$ zwei $K$-Vektorräume. Es gibt eine \enquote{universelle bilineare Abbildung}, d.h. es gibt ein Vektorraum $T$ und eine bilineare Abbildung $\tau: V\times W\to T$, sodass \textit{jede} bilineare Abbildung $\phi: V\times W \to X$ sich schreiben lässt als $f\circ\tau$ mit einer eindeutig bestimmten linearen Abbildung $f: T\to X$.
\begin{tikzcd}
V\times W \ar[r,"\tau"] \ar[rd,"\phi"'] & T \ar[d, dotted, "\exists! f"]\\
& X
\end{tikzcd}

\medbreak
$T$ und $\tau$ sind in der Tat eindeutig bestimmt bis auf einen eindeutigen Isomorphismus, d.h. wenn $\tau':V\times W\to T'$ eine weitere universelle bilineare Abbildung ist, dann gibt es genau einen Isomorphismus $\alpha:T\to T'$, sodass $\alpha(\tau(v,w)) = \tau'(v,w)$ gilt. Man schreibt üblicherweise $V\otimes W$ statt $T$ und $v\otimes w$ statt $\tau(v,w)$ für diesen eindeutig bestimmten Vektorraum und bilineare Abbildung und nennt sie \emph{Tensorprodukt von $V$ und $W$}. (Achtung: Das Tensorprodukt ist streng genommen die Kombination aus $T$ und $\tau$, nicht nur $T$).
\end{theoremdef}

\begin{remark}
Ausgeschrieben sagt die universelle Eigenschaft folgendes: Von einer Abbildungsvorschrift $\phi: (v,w) \mapsto x_{v,w}$, die je einen Inputvektor aus $V$ und einen aus $W$ nimmt und einen Vektor aus $X$ produziert, gibt es genau dann eine Realisierung dieser Abbildungsvorschrift als lineare Abbildung $f: V\otimes W\to X$ mit $f(v\otimes w) = x_{v,w}$, wenn $\phi$ bilinear ist.

Jede bilineare Abbildung kann so eindeutig als lineare Abbildung auf dem Tensorprodukt der beiden Inputräume betrachtet werden, und umgekehrt kann jede lineare Abbildung auf einem Tensorprodukt als bilineare Abbildung aufgefasst werden, indem man sie auf die Menge der reinen Tensoren einschränkt.
\end{remark}

\begin{definition}[Reine Tensoren]\label{tensoren:reine_tensoren}
Ein Element von $V\otimes W$, das die Form $v\otimes w$ hat, wird als \emph{reiner Tensor} bezeichnet.
\end{definition}

\begin{remark}[Diskussion: reine vs. nicht-reine Tensoren]
Ein wichtiger, vielleicht der wichtigste Grund, Tensorräume als eigenständiges Objekt einzuführen statt ausschließlich mit multilinearen Abbildungen zu arbeiten ist die Existenz von nicht-reinen Tensoren: Eine Summe $v_1\otimes w_1 + v_2\otimes w_2 + \ldots + v_k\otimes w_k$ (die natürlich immer existiert, weil $V\otimes W$ ja ein Vektorraum ist) ist i.A. kein reiner Tensor, lässt sich also i.A. nicht darstellen als $v_1\otimes w_1 + v_2\otimes w_2 + \ldots + v_k\otimes w_k = x\otimes y$.

\medbreak
Es stellt sich heraus, dass viele der wirklich interessanten Tensoren, die einem so in freier Wildbahn begegnen, nicht rein sind.

Beispiel: In der Quantenmechanik wird das Tensorprodukt benutzt, um mehrere interagierende Quanten-Systeme als ein einziges großes System zu betrachten: Wenn $V$ und $W$ der Zustandsraum je einer Menge $X$ bzw. $Y$ quantenmechanischer Teilchen sind, dann ist $V\otimes W$ der Zustandsraum des quantenmechanischen Systems, das aus den Teilchen von $X$ und $Y$ besteht und beliebige Interaktionen zwischen ihnen erlaubt. Ein reiner Tensor $v\otimes w$ entspricht in dieser Sichtweise demjenige Zustand des Gesamtsystems, in dem sich die Teilchen aus $X$ im Zustand $v$, und das aus $Y$ im Zustand $w$ befinden. Die nicht-reinen Tensoren entsprechen dann \enquote{Überlagerungen} solcher reinen Zustände und das fundamental wichtige Phänomen von \enquote{verschränkten} Teilchen ist ein Ausdruck dessen, dass eben nicht alle Zustände reine Zustände sind, in denen man die Teilchen von $X$ unabhängig von denen in $Y$ einen Zustand zuschreiben kann: Es kann z.B. ein Zustand der Form $x_1\otimes y_1 + x_2\otimes y_2$ konstruiert (und auch experimentell realisiert) werden, in dem $x_1$ und $x_2$ sowie $y_1$ und $y_2$ beide jeweils senkrecht zueinander sind. In solch einem Zustand des Gesamtsystems können die $X$-Teilchen in Zustand $x_1$ oder $x_2$ gemessen werden, aber nur dann, wenn gleichzeitig die $Y$-Teilchen in Zustand $y_1$ bzw. $y_2$ liegen. Es kann eben nicht unabhängig voneinander der $X$-Anteil in Zustand $x_1$ sein, während der $Y$-Anteil des Gesamtsystems in Zustand $y_2$ ist (hier geht die Orthogonalität ein).
\end{remark}

\begin{example}[Casimir-Element]\label{tensoren:casimir_element}
Ist $V$ ein endlich-dimensionaler $\IR$-Vektorraum und haben wir ein Skalarprodukt fest gewählt, so gibt es einen besonderen Tensor $\Omega_V \in V\otimes V$. Für \textit{jede} Orthogonalbasis $e_1, \ldots, e_n$ von $V$ lässt er sich schreiben als
\[\Omega_V = \sum_{i=1}^n e_i \otimes e_i\]
Für $V=\IR^3$ und die Standardbasis $e_x, ey, e_z$ ist etwa
\[\Omega_{\IR^3} = e_x\otimes e_x + e_y\otimes e_y + e_z\otimes e_z\]

Betrachten wir den Raum der linearen Differentialoperatoren erster Ordnung auf $C^\infty(V)$, d.h. der Vektorraum der Operatoren $\set{\partial_v | v\in V}$. Darin schreibt sich das Casimir-Element als
\[\Omega = \sum_{i=1}^n \partial_{e_i} \otimes \partial_{e_i}\]
Wenn wir die Komposition von Operatoren (eine bilineare Abbildung) auf diesen Tensor anwenden, erhalten wir den Laplace-Operator:
\[\Delta = \sum_{i=1}^n \partial_{e_i}^2\]

Man beachte insbesondere, dass das Ergebnis der Summe auf der rechten Seite unabhängig von der gewählten Basis ist, obwohl die einzelnen Summanden es natürlich nicht sind. Das erklärt z.B., wieso der Laplace-Operator und viele ähnlich aussehende Konstruktionen physikalische sinnvolle Objekte liefern, obwohl ihre Definition auf den ersten Blick basisabhängig zu sein scheint.
\end{example}

\begin{remark}
Man kann sich jedoch leicht davon überzeugen, dass die Menge aller Summen von reinen Tensoren tatsächlich das volle Tensorprodukt abdeckt:
\[V\otimes W = \Set{v_1\otimes w_1 + v_2\otimes w_2 + \ldots + v_k\otimes w_k | k\in\IN, v_i\in V, w_i\in W}\]

In der Welt der Quantenmechanik würde man dazu also z.B. sagen: Der Zustandsraum des aus zwei Einzelsystemen kombinierten Systems besteht aus allen Kombinationen von reinen Zuständen der Einzelsysteme sowie allen Überlagerungen dessen.

Es ist i.A. ein sehr schwieriges Problem, einem konkreten Tensor anzusehen, ob er rein ist oder nicht, und wenn nicht, welche reinen Zustände man zusammen addieren muss, um ihn zu erhalten. Nicht einmal die Anzahl der mindestens notwendigen Summanden ist einfach zu finden im Allgemeinen.
\end{remark}

\begin{remark}[Diskussion: Das Wort \enquote{Tensor}]
Im Gegensatz zum Wort \enquote{Vektor}, das zumindestens meistens \enquote{Element eines Vektorraums} bedeutet und selten anders verwendet wird, ist das Wort \enquote{Tensor} etwas überbelegt. Die verschiedenen Verwendungsformen des Worten fallen grob in zwei Kategorien: 1.) \enquote{ich habe keine Ahnung, was Tensoren sind, aber andere Leute benutzen das Wort, also tue ich das auch} und 2.) irgendetwas, das tatsächlich mit Tensorprodukten zu tun hat.

\smallbreak
Zu 1. später mehr, zu 2. nur soviel: \enquote{Tensor} kann sowohl \enquote{Element eines Tensorprodukts von Vektorräumen} bedeuten (so werden wir das Wort verwenden) als auch einen von diversen, verwandten Begriffe, z.B. wird das auch als Kurzform von \udot{Tensorfeld} verwendet. Ein Tensorfeld ist ein Funktion, die jedem Punkt des gerade betrachten geometrischen Raums $X$ (oder Raumzeit) je ein Element $t(x)\in V_x\otimes V_x \otimes V_x \otimes ... $ zuordnet, wobei gewisse Stetigkeitseigenschaften gefordert werden, die für zwei eng beieinander liegende Punkte $x,x'$ fordern, dass $V_x$ und $V_{x'}$ \enquote{im Wesentlichen der gleiche Raum} sind und $t(x),t(x')$ ebenfalls eng zusammen liegen (formal ist das natürlich eine richtige $\epsilon$-$\delta$-artige Definition). Ggf. wird auch nicht nur ein Vektorraum $V_x$ pro Punkt verwendet, sondern mehrere. Je nachdem, welche Zusatzeigenschaften man an solch ein Tensorfeld stellt, wird auch nicht nur das Tensorprodukt der Vektorräume selbst, sondern auch eine vom Tensorprodukt abgeleitete Konstruktion betrachtet (z.B. symmetrische oder äußere Potenzen, siehe weiter unten).
\end{remark}

\subsection{Elementare Eigenschaften des Tensorprodukts}

\begin{lemma}[{$K\otimes V = V$}]
Ist $V$ ein $K$-Vektorraum, so ist
\[\left\{\begin{array}{rcl}
K\otimes V & \leftrightarrows & V \\
\alpha\otimes v & \mapsto & \alpha v \\
1\otimes v & \reflectbox{$\mapsto$} & v
\end{array}\right.\]
ein paar zueinander inverser Isomorphismen.

Wir erlauben uns, wann immer es uns nützlich erscheint, $K\otimes V$ als identisch zu $V$ zu betrachten. Dabei meinen wir immer die obige Identifizierung.
\end{lemma}

\begin{remark}
Da das Tensorprodukt für alle Paare von $K$-Vektorräumen definiert ist und selbst wieder ein $K$-Vektorraum ist, können wir natürlich auch Tensorprodukte von Tensorprodukten bilden. Es stellt sich die Frage, ob es eine Rolle spielt, wie genau wir das tun.
\end{remark}

\begin{theoremdef}[Assoziativität des Tensorprodukts \& Höhere Tensorgrade]
Für alle $K$-Vektorräume $V_1, V_2, V_3$ ist
\[\begin{array}{rcl}
V_1\otimes(V_2\otimes V_3) &\to& (V_1\otimes V_2)\otimes V_3 \\
v_1\otimes (v_2\otimes v_3) &\mapsto& (v_1\otimes v_2)\otimes v_3
\end{array}\]
ein Isomorphismus.

Wir erlauben es uns, aufgrund der Natürlichkeit dieses Isomorphismus, Klammern in mehrfachen Tensorprodukten wegzulassen und kurz $V_1\otimes V_2\otimes V_3\otimes\cdots\otimes V_k$ zu schreiben. Der Kürze halber definieren wir die \emph{Tensorpotenzen} 
\[V^{\otimes k} := \begin{cases} K & k=0 \\ 
\underbrace{V\otimes V\otimes\cdots\otimes V}_{k\text{-mal}} & k>0\end{cases}\]
\end{theoremdef}

\begin{remark}
Mit dieser Notation gelten dann (abzüglich eben jener natürlicher Isomorphismen) die \enquote{Potenzgesetze}:
\[V^{\otimes n}\otimes V^{\otimes m} = V^{\otimes(n+m)}\]
\[(V^{\otimes n})^{\otimes m} = V^{\otimes nm}\]
\end{remark}

\subsection{Index-Notation - Was das ist und wie man sie loswird}

\begin{remark}
Alle vorher angedeuteten Aussagen über multilineare Abbildungen kann man auf Sätze über lineare Abbildungen und Tensorprodukte zurückführen, z.B.
\end{remark}

\begin{lemma}[Dimension von Tensorprodukten\footnote{Wer mit unendlich-dimensionalen Vektorräumen nicht gut genug vertraut ist, denke sich ein \enquote{für endlich-dimensionale Vektorräume} hinzu.}]\label{tensorprodukt:dimension}
Es gilt $\dim(V\otimes W) = \dim(V)\cdot\dim(W)$.

Präziser: Für jede Basis $(b_i)_{i\in I}$ von $V$ und $(c_j)_{j\in J}$ von $W$ ist $(v_i\otimes w_j)_{i\in I, j\in J}$ eine Basis von $V\otimes W$.
\end{lemma}

\begin{remark}
Insbesondere heißt das, dass jeder Tensor $t\in V\otimes W$ eine eindeutige Koordinaten-Darstellung bzgl. solch einer Basis hat, d.h. es gibt eindeutig bestimmte Skalare $t_{ij}$ sodass
\[t = \sum_{i\in I, j\in J} t_{ij} b_i\otimes c_j\]
gilt und umgekehrt liefert jede Wahl von $\abs{I}\times \abs{J}$ vielen Skalaren $t_{ij}$ auf diese Weise genau einen Tensor.

Hier kommen wir auf die \enquote{falsche} Sichtweise auf Tensoren zurück: Es hat sich bei gewissen Leuten eingebürgert, \enquote{Tensor} austauschbar mit \enquote{Ansammlungen von Zahlen, die mehrere Indizes haben} zu verwenden. Trotz der eben getätigten Feststellung, dass in der Tensorprodukte genau wie alle anderen Vektorräume nach der Wahl einer Basis durch Koordinaten beschrieben werden können, kann kaum überschätzt werden, wie kontraproduktiv diese Sichtweise auf Tensoren ist. Diese Sichtweise ist insbesondere bei Anwendern von Mathematik -- Physikern, Ingenieuren, ... -- besonders beliebt.
\end{remark}

\begin{lemma}
Sind $f:V\to W, g: V'\to W'$ zwei lineare Abbildungen zwischen $K$-Vektorräumen, so gibt es genau eine lineare Abbildung $V\otimes V'  \to W\otimes W'$, die $v\otimes v'$ auf $f(v)\otimes g(v')$ abbildet. Diese Abbildung wird der Bequemlichkeit halber auch als $f\otimes g$ bezeichnet.
\end{lemma}

\begin{remark}
Sind die Dimensionen endlich und hat man Basen der jeweiligen Vektorräume gewählt, so kann man $f$ und $g$ durch ihre Darstellungsmatrix angeben. Die Darstellungsmatrix von $f\otimes g$ ist dann das sogenannte \enquote{Kronecker-Produkt} der Matrizen $F$ und $G$. Je nachdem, wie man die Tensorbasis $\set{b_i\otimes c_j}$ anordnet (zeilen- oder spaltenweise durchnummerieren), ist das Kronecker-Produkt durch
\[F\otimes G = \begin{pmatrix}
f_{11} G & f_{12} G & \cdots & f_{1m} G \\
f_{21} G & f_{22} G & \cdots & f_{2m} G \\
\vdots G &  \vdots  & \ddots & \vdots   \\
f_{n1} G & f_{n2} G & \cdots & f_{nm} G
\end{pmatrix} \quad\text{oder}\quad \begin{pmatrix}
F g_{11} & F g_{12} & \cdots & F g_{1m'} \\
F g_{21} & F g_{22} & \cdots & F g_{2m'} \\
  \vdots &   \vdots & \ddots &   \vdots  \\
F g_{n'1} & F g_{n'2} & \cdots & F g_{n'm'}
\end{pmatrix}\]
gegeben.
\end{remark}

%\subsection{The notion of meaningfull linear operations with tensors - light introduction to invariant theory}
%In the context of our physical world, we can use tensors to measure quantities. A possibly familiar example is stress within a material or gas, which is measured with a tensor of second order.
%The fundamental property we expect from all quantities that we measure, is that they are independent of the orientation of the orthonormal coordinate system - rotating, inverting or mirroring the basis vectors should not change the value of the properties we are interested in (though this may change the numbers with which we express that quantity).
%A velocity vector for example should always point in the same direction, regardless of wether its coordinates are $(3,4,0)^T$ or $(0,0,5)^T$. The same goes for its length or the determinant of a matrix.
%
%%The first question we need to answer is: How does a length preserving change of the coordinate system such as a rotation act on a tensor?
%\begin{definition}[Orthogonal Group $O^3$]
%	Let $O^3$ be the set of all orthogonal matrices \[O^3:=\{r\in\IR^{3\times 3}: |\det{r}| = 1 \}.\]
%	All of the operations described above can be described with a corresponding matrix in $O^3$. Furthermore, equipped with the typical matrix-matrix multiplication, $O^3$ fullfills all requirements of a group - hence the name \emphName{orthogonal group}. $O^3$ has infinitely many elements, but is still compact - allowing for the notion of taking an average over the whole group.
%\end{definition}
%
%\begin{definition}[Action of $O^3$ on the tensor space]
%	An element $r\in O^3$ acts on a vector $v\in\IR^3$ via the matrix-vector product $r\cdot v$, resulting in the rotation or mirroring of the given vector. This is extended to tensors - An element $r\in O^3$ acts on a tensor in $\mathcal{T}$ through the matrix-vector multiplication of each factor of that tensor:
%	\begin{align*}
%		r(\cdot):\mathcal{T} &\to \mathcal{T}\\
%		v_1 \otimes \cdots \otimes v_n&\mapsto r(v_1 \otimes \cdots \otimes v_n) :=r\cdot v_1 \otimes \cdots \otimes r\cdot v_n.
%	\end{align*}
%\end{definition}
%
%\begin{definition}[Orthogonal Group $O^2_{\vc{n}}$ as subgroup of $O^3$]
%	Let $\vc{n}\in \IR^3$. $O^2_{\vc{n}}$ is the subgroup of $O^3$, that acts like the identity on $\vc{n}$,  \[O^2_{\vc{n}}:=\{r\in O^3 | r(\vc{n}) = \vc{n} \}.\]
%	$O^2_{\vc{n}}$  is the embedding of $\IR^2$ and $O^2:= \{r\in\IR^{2\times 2}: |\det r| = 1\}$ as a plane $E$ into $\IR^3$ with $E\perp\vc{n}$.
%\end{definition}
%
%
%\begin{definition}[$O^3$-Invariant linear map of the tensor space]
%	A linear map $f: \mathcal{T}\to \mathcal{T}$ is called \emphName{compatible (or invariant) with respect to $O^3$}, iff for all actions $r\in O^3$  and for any tensor $\ten{T}\in\mathcal{T}$ the relationship
%	\[f \circ r (\ten{T}) = r \circ f (\ten{T})\]
%	holds. This precisely expresses the above notion of independency (or invariancy) of a quantity or tensor of the choice of coordinate system.
%\end{definition}
%Finding $O^3$-invariant linear maps and being sure, that one has found all possibilities, requires some very interesting and complicated piece of mathematics known as \emphName{invariant theory}. This goes far beyond the scope of this thesis, we will restrict ourselves to introducing and applying the results. The interested reader may start by inquiring for the \emphName{Brauer algebra}.
%
%\subsubsection{Operations on one tensor}\label{chap:tensorOperations}
%Below is a list of all types of $O^3$-invariant linear maps between tensors, defined with the elementary tensor $v_1 \otimes \cdots \otimes v_n$. All combinations of them is the full set of of $O^3$-invariant linear maps between tensors:
%\begin{description}
%	\item[Multiplication with a scalar $s\in\IR$.] This does not change the degree of the tensor.
%
%	\item[Permutation of the factors of a tensor.] We have seen this in tensors of second degree, where we called it transpose in correspondence to matrices. Formally, $\sigma \in S_n$ acts on a tensor $v_1 \otimes \cdots \otimes v_n$:
%	\[\sigma(\cdot): \mathcal{T}\to \mathcal{T}, v_1 \otimes \cdots \otimes v_n \mapsto v_{\sigma^{-1}(1)} \otimes \cdots \otimes v_{\sigma^{-1}(n)}\]
%	This does not change the degree of the tensor or any of the vectors $v_i$, it only reorders them. When using index notation, this operation is represented by a permutation of indices.
%
%	\item[Taking the trace $tr$ between two factors of a tensor.] We can pick two vectors of a tensor and replace them with their scalar product:
%	\[tr_{i;j} (\cdot): \mathcal{T} \to \mathcal{T}, v_1 \otimes \cdots \otimes v_i \otimes \cdots \otimes v_j \otimes \cdots \otimes v_n \mapsto \braket{v_i,v_j} v_1 \otimes \cdots \otimes v_{i-1}\otimes v_{i+1} \otimes \cdots \otimes v_{j-1} \otimes v_{j+1} \otimes \cdots \otimes v_n\]
%	This reduces the degree of the tensor by $2$. When using index notation, this operation is represented by the sum over two identical indices, which is often abbreviated with Einstein's sum convention.
%
%	\item[Tensor multiplication with the Casimir element $\Omega$.]
%	\[\ten{\Omega} (\cdot):  \mathcal{T}\to \mathcal{T}, v_1 \otimes \cdots \otimes v_n \mapsto \Omega \otimes v_{1} \otimes \cdots \otimes v_{n} \]
%	This increases the degree of the tensor by $2$. When using index notation, this operation is represented by the multiplication with $\delta_{ij}$.
%\end{description}
%
%\subsubsection{Operations between two tensors}
%Given two tensors $u,v \in \mathcal{T}$, the question of finding invariant linear maps with respect to $O^3$ $\mathcal{T} \times \mathcal{T} \to \mathcal{T}$ is easily reduced to the already described case of operations of one tensor. We just have to identify all possible maps $\mathcal{T} \otimes \mathcal{T} \to \mathcal{T} = \mathcal{T} \to \mathcal{T}$ due to the isomorphism $\mathcal{T} \times \mathcal{T} \to \mathcal{T} \cong \mathcal{T} \otimes \mathcal{T} \to \mathcal{T}$ for bilinear and linear maps. To be more specific: Given two tensors $u,v$, take the tensor product $u\otimes v$ and now aply any combination of the operations described for one tensor. Nevertheless, some operations are very common and worth mentioning.
%
%\begin{definition}[Contraction between two tensors]
%	Taking one or multiple traces between two tensors is given a special name: Contraction.
%	\begin{align*}
%		tr_{i;j} (\cdot,\cdot): \mathcal{T} &\times \mathcal{T} \to \mathcal{T},
%		\\
%		v_1 \otimes \cdots \otimes v_i \otimes \cdots\otimes v_n &\times u_1 \otimes \cdots \otimes u_i \otimes u_j \otimes \cdots \otimes u_n
%		\\
%		\mapsto \braket{v_i,u_j} v_1 \otimes \cdots \otimes v_{i-1}\otimes v_{i+1} \otimes \cdots \otimes v_n &\otimes u_1 \otimes u_{j-1} \otimes u_{j+1} \otimes \cdots \otimes u_n
%	\end{align*}
%\end{definition}
%\begin{definition}[Standard scalar product]
%	The standard scalar product $\braket{\cdot,\cdot}$ between two tensors of same order is defined as the (ordered) full contraction between the two tensors:
%	\begin{align*}
%		\braket{\cdot,\cdot} &: \  V^{\otimes n} \times V^{\otimes n} \to \IR,\\
%		\braket{u_1 \otimes \cdots \otimes u_n,v_1 \otimes \cdots \otimes v_n} &\mapsto tr_{1,\cdots,n;1,\cdots,n} (u_1 \otimes \cdots \otimes u_n,v_1 \otimes \cdots \otimes v_n) \\
%		&= \braket{u_1,v_1}\cdot\braket{u_2,v_2} \cdots \braket{u_n,v_n}.
%	\end{align*}
%\end{definition}
%\subsection{Symmetric tensor space}
%\begin{definition}[Symmetric tensors]
%	A tensor $v$ of $n$-th order is called symmetric, iff for all permutations $\sigma \in S_n$ holds:
%	\[\sigma(v) = v.\]
%\end{definition}
%The space spanned by all symmetric tensors of degree $n$, $\Sym{n}$, is an invariant subspace of the tensor space due to the completely decoupled nature of the actions of $S_n$ and $O^3$ on a tensor. We can take any tensor and project it onto this subspace by taking the average of all possible permutations:
%\begin{align*}
%	q: {\IR^3}^{\otimes n} &\to \Sym{n}\\
%	q\left(v_1 \otimes \cdots  \otimes v_n\right) &\mapsto \frac{1}{n!} \sum_{\sigma \in S_n} v_{\sigma(1)} \otimes \cdots \otimes v_{\sigma(n)}
%\end{align*}
%and linear extension. In index notation this is denoted by round brackets surrounding the indices. $\Sym{n}$ is of dimension $\frac{1}{2}(n+1)(n+2)$.
%
%%\begin{note}
%%	It is easy to show for the standard scalar product on $\IR^{\otimes n}$, that for all $v,u \in \IR^{\otimes n}$ the following identity holds:
%%	\begin{equation}
%%		\braket{q(v),q(u)} = \braket{v,q(u)} = \braket{q(v),u}.
%%		\label{eq:scalarqprod}
%%	\end{equation}
%%\end{note}
%\begin{definition}[Trace and contraction of symmetric tensors]
%	Since for symmetric tensors all the positions $i,j (i\neq j)$ of the two vectors that are used to take the trace $tr_{i;j}$ (contraction) are equivalent to each other, we can use a notation that only shows the relevant information: The \emphProp{number} of traces (contractions) taken of one tensor (between two tensors). \\
%	$k$-fold trace of a symmetric tensor:
%	\begin{align*}
%		tr^{k} (\cdot): \Sym{n} &\to \Sym{n-2k},\\
%		tr^{k} (s) & \mapsto tr_{1,3,5\cdots, 2k-1;2,4,6,\cdots,2k} (s)
%	\end{align*}
%	$k$-fold contraction between two symmetric tensors:
%	\begin{align*}
%		tr^{k} (\cdot,\cdot): \Sym{n}\times\Sym{m} &\to \Sym{n-k}\otimes\Sym{m-k},\\
%		tr^{k} (s,t) & \mapsto tr_{1,2,3\cdots, k;1,2,3\cdots,k} (s,t)
%	\end{align*}
%\end{definition}
%
%Due to the polynomial nature of the ansatz for the distribution function, we will almost exclusively work with symmetric tensor spaces (reference chapter \ref{chap:isomorphismPolyTensor} to see why). Casually spoken, $O^3$ imposes a specific structure on $\Sym{n}$, which brings us to the next section.
%
%\subsection{Level 1 invariant theory: Irreducable and invariant subspaces of the symmetric tensor space}
%\begin{definition}[Invariant subspaces] A subspace $U$ of any vector space $V$ is called \emphName{invariant} with respect to the action of a group $G$, or $G$-invariant, iff for all $r\in G$ and for all $u\in U$, the action of $r$ on $u$ still is in $U$, $r(u)\in U$ or equivilantly: $\forall r\in G: r(U)=U$.
%\end{definition}
%
%\begin{example}[$O^2_{\vc{n}}$-invariant subspaces of $\IR^3$]
%	We pick $\IR^3$ as vector space and the subgroup $O^2_{\vc{n}} \subset O^3$ along with a direction $n\in\IR^3$, for which by definition $\forall r \in O^2_{\vc{n}}: r(n)=n$.
%	Then the subspace $\spn{\vc{n}}= \{\alpha \vc{n}|\alpha\in\IR\}$ is a one-dimensional invariant subspace, since
%	\[\forall r \in O^2_{\vc{n}}, \forall \alpha\in\IR: r(\alpha \vc{n}) = \alpha r(\vc{n}) = \alpha\vc{n}.\]
%	Given an $O^3$-compatible scalar product, we can define the plane $E\subset\IR^3$ orthogonal to $\vc{n}$ and decompose $\IR^3$ into \[\IR^3 = \spn{\vc{n}}\bigoplus E.\] It is easily shown that $E$ is also an invariant, 2-dimensional subspace under the action of $O^2_{\vc{n}}$.
%
%	% a general vector $\vc{v}\in\IR^3$ into \[\vc{v}=\underbrace{\vc{v}-\braket{\vc{v},\vc{n}}\vc{n}}_{\in E} + \braket{\vc{v},\vc{n}}\vc{n} =: \vc{v}_{E} + \vc{v}_{\vc{n}}\]
%\end{example}
%
%\begin{example}[$O^3$-invariant subspaces of the tensor space]
%	These are subspaces $U\subset\mathcal{T}$ that fullfill $\forall r\in O^3: r(U)=U$. One such subspace is the above described space of symmetric tensors, which is of relatively large dimensionality for high $n$. Another example of an invariant subspace would be $\spn{\Omega}\subset\Sym{2}$ with a dimensionality of $1$.
%	One may wonder, if $\Sym{n}$ can be further decomposed into "smallest", invariant subspaces, and if such a decomposition is unique.
%\end{example}
%\begin{definition}[Irreducable (and invariant) subspaces]
%	A subspace $U$ of any vector space $V$ is called \emphName{irreducable} with respect to the action of a group $G$, or \emphName{$G$-irreducable}, iff it is invariant and there is no invariant subspace $W\subset U$ with the properties $W\neq U$ and $W\neq 0$. If a subspace $U$ is invariant and $\dim U=1$, then it is also irreducable, however an irreducable subspace can be of dimension larger than $1$.
%
%	Two irreducable subspaces $U_1, U_2, U_1 \neq U_2$ of a vector space $V$ are orthogonal to each other with respect to any scalar product of $V$ that is compatible with the action of the group $G$. We can project from the full vector space onto an irreducable subspace. Due to the universal orthogonality, such a projection is always unique up to a constant (or a zero).
%
%\end{definition}
%\begin{example}[$O^2_{\vc{n}}$-irreducable subspaces of $\IR^3$]
%	We already know that $E$ and $\spn{\vc{n}}$ are invariant subspaces. Since $\dim{\spn{\vc{n}}}=1$, $\spn{\vc{n}}$ is also an irreducable subspace.
%	For the 2-dimensional $E$ we cannot find a lower-dimensional invariant subspace, since \[\exists r\in O^2_{\vc{n}}: \forall \vc{v}\neq 0 \in E, \forall \alpha\in\IR: r(\vc{v})\neq \alpha \vc{v}.\] This means that $E$ is irreducable as well and that we have found all $O^2_{\vc{n}}$-irreducable subspaces of $\IR^3$.
%\end{example}
%\begin{example}[$O^2_{\vc{n}}$-irreducable subspaces of $\Sym{n}$]
%	We note:
%	\[\Sym{n} = q\left(\left(\IR^3\right)^{\otimes n}\right).\]
%	Insert the splitting of $\IR^3$ into $E$ and $\spn{\vc{n}}$:
%	\begin{align*}
%		\Sym{n} &= q\left(\left(E\bigoplus\spn{\vc{n}}\right)^{\otimes n}\right) \\
%		&=\bigoplus_{i=0}^n q\left(E^{\otimes i}\otimes \left(\spn{\vc{n}}\right)^{\otimes n-i}\right)
%	\end{align*}
%	The orthogonality between $E$ and $\spn{\vc{n}}$ leads to an orthogonality between each of these tensor spaces, the invariancy of each $q\left(E^{\otimes i}\otimes \left(\spn{\vc{n}}\right)^{\otimes n-i}\right)$ follows from the invariancy of the "building blocks" $E$ and $\spn{\vc{n}}$. However, for general $n,i$ these subspaces are not irreducable. For instance we find that the invariant tensor $q(\Omega_E^{\otimes j})$ spans a 1-dimensional (and therefore irreducable) subspace  of $q(E^{\otimes 2j})$. In general, since $q\left(E^{\otimes i}\otimes \left(\spn{\vc{n}}\right)^{\otimes n-i}\right)\cong q(E^{\otimes i})\cong\sym{i}$, finding all irreducable subspaces of $\sym{i}$ immediately leads to all irreducable subspaces of $q\left(E^{\otimes i}\otimes \left(\spn{\vc{n}}\right)^{\otimes n-i}\right)$.
%\end{example}
%\begin{remark}[Invariant tensors]
%	Sometimes one may work with a tensor $\ten{T}$ that is invariant with respect to a group $G$, i.e. $\forall r\in G: r(T) = T$. For example:
%	\begin{itemize}
%		\item $G=S_n$. Then all tensors $\ten{T}\in\Sym{n}$ are invariant
%		\item $G=O^3$. Then all tensors $\ten{T}\in(\IR^3)^{\otimes 2i}$ that are a linear combination of permutations $\Omega^{\otimes i}$ are invariant. If in addition $\ten{T}\in\Sym{2i}$: $\ten{T}=\alpha q\left(\Omega^{\otimes i}\right), \alpha\in\IR$.
%		\item $G=O^2_{\vc{n}}$ and $\ten{T}\in\Sym{n}$. Not all tensors in $q\left(E^{\otimes m}\right)$ are invariant, in fact the only invariant tensors are $\alpha q\left(\Omega_E^{\otimes i}\right), \alpha\in\IR\implies m=2i$. All tensors in $\spn{\vc{n}}$ are invariant, so an $O^2_{\vc{n}}$-invariant, symmetric tensor must come from the subspace spanned by $\left\{q\left(\Omega_E^{\otimes i}\otimes \vc{n}^{\otimes n-2i}\right)\right\}$, i.e.
%		\begin{equation}
%			\ten{T}=\sum_{i=0}^{\frac{n}{2}} \alpha_i q\left(\Omega_E^{\otimes i}\vc{n}^{\otimes n-2i}\right), \alpha_i \in \IR.
%			\label{eq:O2invariantTensors}
%		\end{equation}
%	\end{itemize}
%	Notice, that not all tensors from an invariant subspace are themselves invariant. Even that a tensor lies in an irreducable subspace is no guarantee that the tensor itself is invariant.
%\end{remark}
%
%\begin{example}[$O^3$-irreducable subspaces of $\Sym{n}$]
%	It turns out, that $\Sym{n}$ can be expressed as the direct sum of $\left\lfloor\frac{n}{2}\right\rfloor+1$ $O^3$-irreducable subspaces (i.e. $\Sym{n}$ is \emphName{semi-simple}). Most of these subspaces are of dimensionality greater than $1$. Furthermore, these irreducable subspaces are \emphProp{pairwise orthogonal} with respect to \emphProp{any} scalar product of the $\Sym{n}$ that is compatible with $O^3$ (which is a rather redundant requirement, since we wouldn't want to choose a scalar product that is not compatible with $O^3$ anyway).
%
%	For every order $n$, we find that the largest irreducable subspace of $\Sym{n}$ consists of trace-free tensors.
%	\begin{definition}[Symmetric trace-free tensors]
%		A symmetric tensor $v$ of order $n>1$ is called symmetric trace-free, iff $tr_{1,2}(v)=0$. Tensors of order 1 are always symmetric tracefree.
%		The space spanned by all symmetric trace-free tensors of order $n$ is denoted as $\STF{n}$ and has dimension $2n+1$. On $\STF{n}$ all $O^3$-compatible scalar products of the tensor space are equivalent to each other (they give the same result up to a constant factor). That has the effect that an orthogonal basis of $\STF{n}$ chosen with respect to one scalar product is orthogonal with respect to any other scalar product (as long as both are $O^3$-compatible). In index notation, the projection onto the space of symmetric trace-free tensor is denoted as square brackets around the indices.
%	\end{definition}
%	The other irreducable subspaces are related to symmetric tracefree tensors:
%	\begin{definition}[Symmetric traceable tensors]
%		$\ST{n,a}$ is the space of symmetric tensors of order $n+2a$, that result in a trace-free tensor (of order $n$) after taking $i$ traces. Equivilantly: $\ST{n,a}$ is the space of symmetric tensors of order $n+2a$, that can be expressed as the result of taking trace-free tensors (of order $n$), and applying the linear map
%		\[q\circ \underbrace{\ten{\Omega}\circ \cdots \circ \ten{\Omega}}_{i \text{ times}}.\]
%		This means that there is a bijective map between $\ST{n,i}$ and $\STF{n}$, i.e. $\ST{n,i}$ and $\STF{n}$ are isomorph to each other.
%	\end{definition}
%
%	\begin{lemma}[Decomposition of $\Sym{n}$ into its $O^3$ irreducable subspaces]
%		Each $\STF{i}$ is a building block for subspaces of $\Sym{n}, i\leq n$,
%		\[\Sym{n} = \bigoplus_{i=0}^{\lfloor\frac{n}{2}\rfloor} q\left(\Omega^{\otimes i}\otimes \STF{n-2i}\right)=\bigoplus_{i=0}^{\lfloor\frac{n}{2}\rfloor}\ST{n-2i,i}\]
%
%	\end{lemma}
%
%	When building an orthogonal basis for $\Sym{n}$ for a physical problem (i.e. $O^3$ acts on $\Sym{n}$), the natural choice is the composition of all orthonormal basis sets from each of the subspaces.
%
%\end{example}
%
%\subsection{Level 2 invariant theory: Linear and Bilinear Maps between Irreducable, invariant subspaces of the symmetric tensor space}
%\begin{lemma}[Compatible linear Maps between irreducable subspaces (Schur's Lemma)]
%	Let $U_1$ and $U_2$ be two invariant, irreducable subspaces of a vector space. A compatible linear map $\phi: U_1 \to U_2$ is only non-zero, iff $U_1$ and $U_2$ are isomorph to each other. All non-zero linear maps $U_1 \to U_2$ are the same up to a constant.
%\end{lemma}
%Applied to the space of all symmetric tensors, we find non-zero, $O^3$-compatible linear maps only of the form $\ST{n,a}\to\ST{n,b}, n,a,b \in \IN_0$.
%\begin{lemma}[$O^3$-compatible, bilinear maps between symmetric trace free tensors]
%	An $O^3$-compatible bilinear map $\phi^{n\tilde{n}\hat{n}}: \STF{\tilde{n}}\times\STF{\hat{n}}\to\STF{n}$ can only be non-zero, iff the following conditions apply:
%	\begin{align*}
%		\tilde{n}+\hat{n} + n \text{ even}\\
%		|\tilde{n}-\hat{n}|\leq n\leq \tilde{n}+\hat{n}
%	\end{align*}
%	Then $\phi^{n\tilde{n}\hat{n}}$ is unique (up to a constant).
%\end{lemma}
%\begin{proof}
%	$\phi^{n\tilde{n}\hat{n}}$ is isomorph to an $O^3$-compatible linear map $\varphi^{n\tilde{n}\hat{n}}: \STF{\tilde{n}}\otimes\STF{\hat{n}}\to\STF{n}$. $\varphi$ can only be non-zero, iff there exists a subspace of $\STF{\tilde{n}}\otimes\STF{\hat{n}}$, that is isomorph to $\STF{n}$, so we are looking for a projection of $\STF{\tilde{n}}\otimes\STF{\hat{n}}$ onto $\ST{n,i}$ with condition \[n+2i=\tilde{n}+\hat{n} \iff \tilde{n}+\hat{n} + n \text{ even}.\]
%	Such a projection is always linear, unique, and compatible with $O^3$. Due to the isomorphism between $\STF{n}$ and $\STF{n,i}$, this makes $\varphi$ unique up to a constant, if it exists.
%
%	Let $s$ be any tensor of degree $n$, and $t$ be the resulting tensor from the projection of $t$ onto $\STF{n}$. Projecting a tensor of the form $\Omega \otimes s$ (or some permutation) onto $\STF{n+2}$ is equivalent to projecting $t$ onto $\STF{n+2}$. Since the only linear map from $\STF{n}$ to $\STF{n+2}$ is the zero-map, the projection of $\Omega \otimes s$ onto $\STF{n+2}$ will always result in $0$. This imposes the condition
%	\[n\leq \tilde{n}+\hat{n}\]
%	onto $n, \tilde{n},$ and $\hat{n}$.
%
%	In chapter \ref{chap:tensorOperations} we listed all types of linear, $O^3$-compatible maps, $\varphi^{n\tilde{n}\hat{n}}$ must be a combination of these. The only map that decreases the order of a given tensor is taking the trace of that tensor. For a tensor of the specific form $\STF{\tilde{n}}\otimes\STF{\hat{n}}$, taking the trace within the first $\tilde{n}$ or the last $\hat{n}$ factors will always be zero. The only traces that yield a non-zero result are those taken between the first $\tilde{n}$ and the last $\hat{n}$ factors, i.e. $k$-fold contractions between the two arguments of $\phi$. Since the two arguments are each symmetric, all $k$-fold contractions between them are equivalent. The number of traces $k$ with a non-zero result we can take of a tensor from the space $\STF{\tilde{n}}\otimes\STF{\hat{n}}$ is limited by $\tilde{n}$ and $\hat{n}$, $k\leq \min{\tilde{n},\hat{n}}$. Since $n=\tilde{n}+\hat{n}-2k$, that imposes the condition on $n$:
%	\[n \geq \tilde{n}+\hat{n}-2\min(\tilde{n},\hat{n}) = \max(\tilde{n}-\hat{n},\hat{n}-\tilde{n}) = |\tilde{n}-\hat{n}|.\]
%
%	%5For all $n$ fullfilling the above conditions, $\ST{n,i}$ with $i=\frac{\tilde{n}+\hat{n}-n}{2}$ is one of the invariant, irreducable subspaces of $\STF{\tilde{n}}\otimes\STF{\hat{n}}$, i.e. $\varphi$ is a projection (up to a constant) onto $\ST{n,i}$ (or 0), making $$.
%\end{proof}
%\begin{lemma}[$O^3$-compatible, bilinear maps between symmetric tensors]\label{lem:bilinearMapDecomp}
%	Every $O^3$-compatible bilinear map $\psi: \Sym{\tilde{m}}\times\Sym{\hat{m}}\to\Sym{m}$ with $m,\tilde{m},\hat{m}\in\IN_0$  can be expressed as the sum of $O^3$-compatible bilinear maps between all combinations of invariant, irreducable subspaces of $\Sym{\tilde{m}}$, $\Sym{\hat{m}}$, and $\Sym{m}$. With
%	\begin{align*}
%		m&=n+2a, & \tilde{m}&=\tilde{n}+2b, & \hat{m} &= \hat{n} +2c & 	\tilde{n},b,\hat{n},c,n,a &\in \IN_0
%	\end{align*}
%	we can write
%	\begin{align*}
%		\psi: &\Sym{\tilde{m}}\times\Sym{\hat{m}}\to\Sym{m}
%		\\
%		& = \bigoplus_{b=0}^{\lfloor\frac{\tilde{m}}{2}\rfloor}\ST{\tilde{n},b} \times \bigoplus_{c=0}^{\lfloor\frac{\hat{m}}{2}\rfloor}\ST{\hat{n},c} \to \bigoplus_{a=0}^{\lfloor\frac{m}{2}\rfloor}\ST{n,a}
%		\\
%		&=
%		\bigoplus_{b=0}^{\lfloor\frac{\tilde{m}}{2}\rfloor} \bigoplus_{c=0}^{\lfloor\frac{\hat{m}}{2}\rfloor}
%		\bigoplus_{a=0}^{\lfloor\frac{m}{2}\rfloor}
%		\ST{\tilde{n},b} \times \ST{\hat{n},c} \to \ST{n,a}\\
%		&=:\bigoplus_{b=0}^{\lfloor\frac{\tilde{m}}{2}\rfloor} \bigoplus_{c=0}^{\lfloor\frac{\hat{m}}{2}\rfloor}
%		\bigoplus_{a=0}^{\lfloor\frac{m}{2}\rfloor} \psi_{abc}^{n\tilde{n}\hat{n}}
%	\end{align*}
%	Each of the $\psi_{abc}^{n\tilde{n}\hat{n}}$ are unique up to a constant and only non-zero, iff the following conditions apply:
%	\begin{align*}
%		\tilde{n}+\hat{n} + n \text{ even}\\
%		|\tilde{n}-\hat{n}|\leq n\leq \tilde{n}+\hat{n}.
%	\end{align*}
%	Given a set of all maps $\psi_{abc}^{n\tilde{n}\hat{n}}$, two $\psi$ can only differ in the constants multiplied with those maps. Furthermore, due to the isomorphism between $\STF{n}$ and $\ST{n,i}$, all $\psi_{abc}^{n\tilde{n}\hat{n}}$  are isomorph to a map $Q^{n\tilde{n}\hat{n}}: \STF{\tilde{n}}\otimes\STF{\hat{n}}\to\STF{n}$, which is unique up to a constant.
%	Given a set of all maps $Q^{n\tilde{n}\hat{n}}$, two tensors of the form $U:=q\left(\Omega^{\otimes b}\otimes u\right)$ and $V:=q\left(\Omega^{\otimes b}\otimes v\right)$ with $u\in\STF{\tilde{n}},v\in\STF{\hat{n}}$, we can express any $\psi_{abc}^{n\tilde{n}\hat{n}}(U,V)$ as
%
%	%	Given a set of all maps $Q^{n\tilde{n}\hat{n}}$, we can express any $\psi_{abc}^{n\tilde{n}\hat{n}}$ as
%	%	\begin{align*}
%	%	\psi_{abc}^{n\tilde{n}\hat{n}}(u,v) = S_{abc}^{n\tilde{n}\hat{n}} q\circ\underbrace{\ten{\Omega}\circ\cdots\circ\ten{\Omega}}_{a \text{ times}}\circ Q^{n\tilde{n}\hat{n}}(tr^b(u),tr^c(v)), S_{abc}^{n\tilde{n}\hat{n}} \in \IR.
%	%	\end{align*}
%	%	Given two tensors of the form $q\left(\Omega^{\otimes b}\otimes u\right)$ and $q\left(\Omega^{\otimes b}\otimes v\right)$ with $u\in\STF{\tilde{n}},v\in\STF{\hat{n}}$, this simplifies to:
%
%	\begin{align}
%		\psi_{abc}^{n\tilde{n}\hat{n}}\left(q\left(\Omega^{\otimes b}\otimes u\right),q\left(\Omega^{\otimes b}\otimes v\right)\right) = S_{abc}^{n\tilde{n}\hat{n}} \cdot q\left(\Omega^{\otimes a}\otimes Q^{n\tilde{n}\hat{n}}(u,v)\right).
%		\label{eq:bilinearMapDecomp}
%	\end{align}
%
%	\textbf{The only degree of freedom for $O^3$-compatible, bilinear maps $\psi$ between symmetric tensors lies in the choice for the constants $S_{abc}^{n\tilde{n}\hat{n}}$.}
%\end{lemma}
