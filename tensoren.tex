% !TeX root = spherical_harmonics.tex
% !TeX spellcheck = de_DE

\subsection{Das Tensorprodukt}

\begin{remark}
Bilineare Abbildungen sind in vielerlei Hinsicht ähnlich zu linearen Abbildungen, z.B. kann man sie durch Basiswahl eindeutig beschreiben, d.h.

\medbreak
Wenn $V,W$ zwei $K$-Vektorräume sind, $b_1,\ldots,b_n$ eine Basis von $V$ und $c_1, \ldots c_m$ eine Basis von $W$, und $\phi: V\times W\to X$ eine bilineare Abbildung, dann sind alle Werte $\phi(v,w)$ bereits eindeutig festgelegt, wenn man $\phi(b_i,c_j)$ für alle $i$ und $j$ kennt, und umgekehrt liefert jede beliebige Wahl von Vektoren $x_{ij}\in X$ genau eine bilineare Abbildung $\psi: V\times W\to X$, die $\psi(b_i,c_j) = x_{ij}$ erfüllt.

\medbreak
Weiter könnte man z.B. jetzt den Raum aller bilinearen Abbildungen $Bil(V,W;X)$ betrachten und beweisen, dass das selbst ein Vektorraum ist genau wie der Raum der linearen Abbildungen zwischen zwei festen Vektorräumen selbst ein Vektorraum ist.

\medbreak
Ähnliches gilt auch für $k$-lineare Abbildungen. Außerdem kann man sich davon überzeugen, dass diese Strukturen mit Hintereinanderausführung von Abbildungen verträglich sind. Hier ist noch zu beachten, dass multilineare Abbildungen viel mehr Möglichkeiten haben, zwei Abbildungen hintereinander auszuführen, z.B. könnte man zwei bilineare Abbildungen hintereinander ausführen, indem man
\[\phi(\psi(u,v),w)\quad\text{oder}\quad \phi(u,\psi(v,w))\]
bildet und i.A. sind das zwei verschiedene $3$-lineare Abbildungen. Man hat bereits drei fundamental verschiedene Möglichkeiten, lineare und bilineare Abbildungen miteinander zu kombinieren: Man kann $\phi(\alpha(u),v)$, $\phi(u,\alpha(w))$ oder $\alpha(\phi(u,v))$ bilden je nachdem, was davon tatsächlich definiert ist. Alle Kombinationen sind jedoch wieder bilineare Abbildungen. Wenn man allgemein $k$- und $m$-lineare Abbildungen miteinander kombinieren will, hat man sehr schnell eine explodierende Anzahl verschiedener Möglichkeiten vor sich. Sie alle liefern wieder multilineare Abbildungen als Ergebnis und alle Möglichkeiten $a$ verschiedene multilineare Abbildungen auf diese Weise zu kombinieren, sind selbst $a$-fach lineare Operationen zwischen den entsprechenden Abbildungsräumen.

\medbreak
Man könnte all das jetzt für alle diese Kombinationen beweisen, wenn man zu viel Freizeit und Tinte hat. Alle Beweise sind langweilig, wenn man den linearen Fall einmal verstanden hat und funktionieren in der Tat völlig analog.\footnote{Wer's nicht glaubt, probiere es selbst aus und finde alle diese Beweise, bis das notwendige Maß an Langeweile erreicht ist.} Das große Problem ist eigentlich, eine geeignete Notation zu finden, die einem erlaubt, diese Beweise alle nur einmal und dafür allgemein zu führen, statt in jeder neuen Kombination von vorne anfangen zu müssen. Und selbst wenn man sich so eine Notation überlegt, dann steht man noch vor einigen technischen, aber völlig trivialen Problemen, die mehr Arbeit verlangen als man für solche Trivialitäten erwartet. Wenn man beispielsweise eine Notation erfindet um zwei multilineare Abbildungen miteinander zu verbinden, dann stellt sich die Frage, ob alle Arten, drei multilineare Abbildungen zu verbinden, sich durch schrittweises Verbinden von je zweien erhalten lassen und ob dafür eine geeignete Form von Assoziativität gilt. Erneut stellt man fest, dass die Antwort \enquote{ja} ist, die geeignete Form von Assoziativität gilt und die Beweise alle analog zum linearen Fall laufen.
\end{remark}

\begin{remark}
All diese Fälle sind letztendlich so sehr ähnlich zum linearen Fall, dass es in der Tat eine Konstruktion gibt, die es einem erlaubt, multilineare Abbildungen als echte lineare Abbildungen aufzufassen, sodass man überhaupt nichts mehr beweisen muss, das über den (schon bekannten) linearen Fall und die Existenz und grundlegenden Eigenschaften dieser Konstruktion hinaus geht. Dieses Konstrukt ist das \emph{Tensorprodukt}.
\end{remark}

\begin{theoremdef}[Universelle Eigenschaft und Existenz des Tensorprodukts]\label{tensorprodukt:def}
Es seien $V,W$ zwei $K$-Vektorräume. Es gibt eine \enquote{universelle bilineare Abbildung}, d.h. es gibt ein Vektorraum $T$ und eine bilineare Abbildung $\tau: V\times W\to T$, sodass \textit{jede} bilineare Abbildung $\phi: V\times W \to X$ sich schreiben lässt als $f\circ\tau$ mit einer eindeutig bestimmten linearen Abbildung $f: T\to X$.

\begin{tikzcd}
V\times W \ar[r,"\tau"] \ar[rd,"\phi"'] & T \ar[d, dotted, "\exists! f"]\\
& X
\end{tikzcd}

\medbreak
$T$ und $\tau$ sind in der Tat eindeutig bestimmt bis auf einen eindeutigen Isomorphismus, d.h. wenn $\tau':V\times W\to T'$ eine weitere universelle bilineare Abbildung ist, dann gibt es genau einen Isomorphismus $\alpha:T\to T'$, sodass $\alpha(\tau(v,w)) = \tau'(v,w)$ gilt. Man schreibt üblicherweise $V\otimes W$ statt $T$ und $v\otimes w$ statt $\tau(v,w)$ für diesen eindeutig bestimmten Vektorraum und bilineare Abbildung und nennt sie \emph{Tensorprodukt von $V$ und $W$}. (Achtung: Das Tensorprodukt ist streng genommen die Kombination aus $T$ und $\tau$, nicht nur $T$).
\end{theoremdef}

\begin{remark}
Ausgeschrieben sagt die universelle Eigenschaft folgendes: Von einer Abbildungsvorschrift $\phi: (v,w) \mapsto x_{v,w}$, die je einen Inputvektor aus $V$ und einen aus $W$ nimmt und einen Vektor aus $X$ produziert, gibt es genau dann eine Realisierung dieser Abbildungsvorschrift als lineare Abbildung $f: V\otimes W\to X$ mit $f(v\otimes w) = x_{v,w}$, wenn $\phi$ bilinear ist.

Jede bilineare Abbildung kann so eindeutig als lineare Abbildung auf dem Tensorprodukt der beiden Inputräume betrachtet werden, und umgekehrt kann jede lineare Abbildung auf einem Tensorprodukt als bilineare Abbildung aufgefasst werden, indem man sie auf die Menge der reinen Tensoren einschränkt.
\end{remark}

\begin{definition}[Reine Tensoren]\label{tensoren:reine_tensoren}
Ein Element von $V\otimes W$, das die Form $v\otimes w$ hat, wird als \emph{reiner Tensor} bezeichnet.
\end{definition}

\begin{remark}[Diskussion: reine vs. nicht-reine Tensoren]
Ein wichtiger, vielleicht der wichtigste Grund, Tensorräume als eigenständiges Objekt einzuführen statt ausschließlich mit multilinearen Abbildungen zu arbeiten ist die Existenz von nicht-reinen Tensoren: Eine Summe $v_1\otimes w_1 + v_2\otimes w_2 + \ldots + v_k\otimes w_k$ (die natürlich immer existiert, weil $V\otimes W$ ja ein Vektorraum ist) ist i.A. kein reiner Tensor, lässt sich also i.A. nicht darstellen als $v_1\otimes w_1 + v_2\otimes w_2 + \ldots + v_k\otimes w_k = x\otimes y$.

\medbreak
Es stellt sich heraus, dass viele der wirklich interessanten Tensoren, die einem so in freier Wildbahn begegnen, nicht rein sind.
\end{remark}
\begin{example}
In der Quantenmechanik wird das Tensorprodukt benutzt, um mehrere interagierende Quanten-Systeme als ein einziges großes System zu betrachten: Wenn $V$ und $W$ der Zustandsraum je einer Menge $X$ bzw. $Y$ quantenmechanischer Teilchen sind, dann ist $V\otimes W$ der Zustandsraum des quantenmechanischen Systems, das aus den Teilchen von $X$ und $Y$ besteht und beliebige Interaktionen zwischen ihnen erlaubt. Ein reiner Tensor $v\otimes w$ entspricht in dieser Sichtweise demjenige Zustand des Gesamtsystems, in dem sich die Teilchen aus $X$ im Zustand $v$, und das aus $Y$ im Zustand $w$ befinden. Die nicht-reinen Tensoren entsprechen dann \enquote{Überlagerungen} solcher reinen Zustände und das fundamental wichtige Phänomen von \enquote{verschränkten} Teilchen ist ein Ausdruck dessen, dass eben nicht alle Zustände reine Zustände sind, in denen man die Teilchen von $X$ unabhängig von denen in $Y$ einen Zustand zuschreiben kann: Es kann z.B. ein Zustand der Form $x_1\otimes y_1 + x_2\otimes y_2$ konstruiert (und auch experimentell realisiert) werden, in dem $x_1$ und $x_2$ sowie $y_1$ und $y_2$ beide jeweils senkrecht zueinander sind. In solch einem Zustand des Gesamtsystems können die $X$-Teilchen in Zustand $x_1$ oder $x_2$ gemessen werden, aber nur dann, wenn gleichzeitig die $Y$-Teilchen in Zustand $y_1$ bzw. $y_2$ liegen. Es kann eben nicht unabhängig voneinander der $X$-Anteil in Zustand $x_1$ sein, während der $Y$-Anteil des Gesamtsystems in Zustand $y_2$ ist (hier geht die Orthogonalität ein).
\end{example}

\begin{example}[Casimir\footnote{Hendrik Brugt Gerhard Casimir, 1909 -- 2000, niederländischer Physiker}-Element]\label{tensoren:casimir_element}
Ist $V$ ein endlich-dimensionaler $\IR$-Vektorraum und haben wir ein Skalarprodukt fest gewählt, so gibt es einen besonderen Tensor $\Omega_V \in V\otimes V$, den wir \emph{Casimir-Element} nennen. Für \textit{jede} Orthonormalbasis $e_1, \ldots, e_n$ von $V$ lässt er sich schreiben als
\[\Omega_V = \sum_{i=1}^n e_i \otimes e_i\]
(laut Aufgabe~\ref{tensoren:ex:casimir_wohldef} ist dies wohldefiniert)
Für $V=\IR^3$ und die Standardbasis $e_x, ey, e_z$ ist etwa
\[\Omega_{\IR^3} = e_x\otimes e_x + e_y\otimes e_y + e_z\otimes e_z\]

Betrachten wir den Raum der linearen Differentialoperatoren erster Ordnung auf $C^\infty(V)$, d.h. der Vektorraum der Operatoren $\set{\partial_v | v\in V}$. Darin schreibt sich das Casimir-Element als
\[\Omega = \sum_{i=1}^n \partial_{e_i} \otimes \partial_{e_i}\]
Wenn wir die Komposition von Operatoren (eine bilineare Abbildung) auf diesen Tensor anwenden, erhalten wir den Laplace\footnote{Pierre-Simon, marquis de Laplace, 1749 –- 1827, franz. Gelehrter, Mathematiker, Physiker, Philosoph uvm.}-Operator:
\[\Delta = \sum_{i=1}^n \partial_{e_i}^2\]

Man beachte insbesondere, dass das Ergebnis der Summe auf der rechten Seite unabhängig von der gewählten Basis ist, obwohl die einzelnen Summanden es natürlich nicht sind. Das erklärt z.B., wieso der Laplace-Operator und viele ähnlich aussehende Konstruktionen physikalische sinnvolle Objekte liefern, obwohl ihre Definition auf den ersten Blick basisabhängig zu sein scheint.
\end{example}

\begin{remark}
Man kann sich jedoch leicht davon überzeugen, dass die Menge aller Summen von reinen Tensoren tatsächlich das volle Tensorprodukt abdeckt:
\[V\otimes W = \Set{v_1\otimes w_1 + v_2\otimes w_2 + \ldots + v_k\otimes w_k | k\in\IN, v_i\in V, w_i\in W}\]

In der Welt der Quantenmechanik würde man dazu also z.B. sagen: Der Zustandsraum des aus zwei Einzelsystemen kombinierten Systems besteht aus allen Kombinationen von reinen Zuständen der Einzelsysteme sowie allen Überlagerungen dessen.

Es ist i.A. ein sehr schwieriges Problem, einem konkreten Tensor anzusehen, ob er rein ist oder nicht, und wenn nicht, welche reinen Zustände man zusammen addieren muss, um ihn zu erhalten. Nicht einmal die Anzahl der mindestens notwendigen Summanden ist einfach zu finden im Allgemeinen.
\end{remark}

\begin{remark}[Diskussion: Das Wort \enquote{Tensor}]
Im Gegensatz zum Wort \enquote{Vektor}, das fast immer \enquote{Element eines Vektorraums} bedeutet und selten anders verwendet wird, ist das Wort \enquote{Tensor} etwas überbelegt. Die verschiedenen Verwendungsformen des Wortes fallen grob in zwei Kategorien: 1.) \enquote{ich habe keine Ahnung, was Tensoren sind, aber andere Leute benutzen das Wort, also tue ich das auch} und 2.) irgendetwas, das mit Tensorprodukten zu tun hat.

\smallbreak
Zu 1. später mehr, zu 2. nur soviel: \enquote{Tensor} kann sowohl \enquote{Element eines Tensorprodukts von Vektorräumen} bedeuten (so werden wir das Wort verwenden) als auch einen von diversen, verwandten Begriffe, z.B. wird das auch als Kurzform für \enquote{Operator zwischen Tensorpotenzen $V^{\otimes a}\to V^{\otimes b}$, die mit einer bestimmten Symmetrie verträglich sind} oder \enquote{Elemente eines Tensorprodukts, die unter einer bestimmten Symmetrie invariant sind} oder von \udot{Tensorfeld} verwendet. Ein Tensorfeld ist ein Funktion, die jedem Punkt eines aktuell relevanten geometrischen Raums (oder Raumzeit) $X$ je ein Element $t(x)\in V_x\otimes V_x \otimes V_x \otimes ... $ zuordnet, wobei gewisse Stetigkeitseigenschaften gefordert werden, die für zwei eng beieinander liegende Punkte $x,x'$ fordern, dass $V_x$ und $V_{x'}$ \enquote{im Wesentlichen der gleiche Raum} sind und $t(x),t(x')$ in selbigem eng beieinander liegen (formal ist das natürlich eine richtige $\epsilon$-$\delta$-artige Definition). Ggf. wird auch nicht nur ein Vektorraum $V_x$ pro Punkt verwendet, sondern mehrere. Je nachdem, welche Zusatzeigenschaften man an solch ein Tensorfeld stellt, wird auch nicht nur das Tensorprodukt der Vektorräume selbst, sondern auch eine vom Tensorprodukt abgeleitete Konstruktion betrachtet (z.B. symmetrische Potenzen).
\end{remark}

\subsection{Elementare Eigenschaften des Tensorprodukts}

\begin{lemma}[{$K\otimes V = V$}]
Ist $V$ ein $K$-Vektorraum, so ist
\[\left\{\begin{array}{rcl}
K\otimes V & \leftrightarrows & V \\
\alpha\otimes v & \mapsto & \alpha v \\
1\otimes v & \reflectbox{$\mapsto$} & v
\end{array}\right.\]
ein Paar zueinander inverser, natürlicher Isomorphismen.
\end{lemma}

\begin{remark}
Wir erlauben uns deshalb, wann immer es uns nützlich erscheint, $K\otimes V$ und $V\otimes K$ als identisch zu $V$ zu betrachten. Dabei meinen wir immer die obige Identifizierung.
\end{remark}

\begin{remark}
Da das Tensorprodukt für alle Paare von $K$-Vektorräumen definiert ist und selbst wieder ein $K$-Vektorraum ist, können wir natürlich auch Tensorprodukte von Tensorprodukten bilden. Es stellt sich die Frage, ob es eine Rolle spielt, wie genau wir das tun.
\end{remark}

\begin{lemmadef}[Assoziativität des Tensorprodukts \& Höhere Tensorgrade]
Für alle $K$-Vektorräume $V_1, V_2, V_3$ ist
\[\left\lbrace\begin{array}{rcl}
V_1\otimes(V_2\otimes V_3) &\to& (V_1\otimes V_2)\otimes V_3 \\
v_1\otimes (v_2\otimes v_3) &\mapsto& (v_1\otimes v_2)\otimes v_3
\end{array}\right.\]
ein natürlicher Isomorphismus.

Wir erlauben es uns, aufgrund der Natürlichkeit dieses Isomorphismus, Klammern in mehrfachen Tensorprodukten wegzulassen und kurz $V_1\otimes V_2\otimes V_3\otimes\cdots\otimes V_k$ zu schreiben. Der Kürze halber definieren wir außerdem die \emph{Tensorpotenzen}
\[V^{\otimes k} := \begin{cases} K & k=0 \\ 
\underbrace{V\otimes V\otimes\cdots\otimes V}_{k\text{ Faktoren}} & k>0\end{cases}\]
Man spricht in diesem Zusammenhang auch von Tensoren vom \emph{Grad} $k$.
\end{lemmadef}

\begin{remark}
Mit dieser Notation gelten dann (abzüglich eben jener natürlicher Isomorphismen) die \enquote{Potenzgesetze}:
\[V^{\otimes n}\otimes V^{\otimes m} = V^{\otimes(n+m)}\]
\[(V^{\otimes n})^{\otimes m} = V^{\otimes nm}\]
\end{remark}

\subsection{Index-Notation -- Die Sicht des 19.Jahrhunderts}

\begin{remark}
Alle vorher angedeuteten Aussagen über multilineare Abbildungen kann man auf Sätze über lineare Abbildungen und Tensorprodukte zurückführen, z.B.
\end{remark}

\begin{lemmadef}[Standardskalarprodukt auf Tensorpotenzen]
Haben wir ein Skalarprodukt $\braket{\cdot,\cdot}$ auf $V$ gegeben, ist das \emph{Standardskalarprodukt} (auch \enquote{von $\braket{\cdot,\cdot}$ \emph{induziertes} Skalarprodukt}) auf $V^{\otimes m}$ definiert mit der Vorschrift
\[
\braket{\cdot, \cdot} : 
\left\lbrace \begin{array}{rcl}
	V^{\otimes m} \times V^{\otimes m} &\to& K \\
\braket{v_1 \otimes \cdots \otimes v_m, w_1 \otimes \cdots \otimes w_m} &\mapsto& \braket{v_1,w_1} \cdot \cdots \cdot \braket{v_m, w_m}
\end{array}\right.
\]
und ihrer linearen Fortsetzung.
\end{lemmadef}

\begin{lemma}[Dimension von Tensorprodukten\footnote{Wer mit unendlich-dimensionalen Vektorräumen nicht gut genug vertraut ist, denke sich ein \enquote{für endlich-dimensionale Vektorräume} hinzu.}]\label{tensorprodukt:dimension}
Es gilt $\dim(V\otimes W) = \dim(V)\cdot\dim(W)$.

Präziser: Für jede Basis $(b_i)_{i\in I}$ von $V$ und $(c_j)_{j\in J}$ von $W$ ist $(v_i\otimes w_j)_{i\in I, j\in J}$ eine Basis von $V\otimes W$.
\end{lemma}

\begin{corollary}
Entsprechendes gilt auch für mehrfache Tensorprodukte, insbesondere für Tensorpotenzen: Ist $e_1,\ldots,e_n$ eine Basis von $V$, so ist $\Set{e_{i_1} \otimes e_{i_2} \otimes \cdots \otimes e_{i_k} | i_j\in\set{1,\ldots,n}}$ eine Basis von $V^{\otimes k}$.

Haben wir ein Skalarprodukt auf $V$ gegeben und ist $e_1, \ldots, e_n$ eine Orthonormalbasis von $V$ bzgl. dieses Skalarprodukts, so ist diese Basis des Tensorprodukts auch eine Orthonormalbasis bzgl. des Standardskalarprodukts auf $V^{\otimes k}$.
\end{corollary}

\begin{definition}
Wenn $e_1, \ldots, e_n$ zufällig eine Basis ist, die man \enquote{Standardbasis} nennt, dann nennt man $\Set{e_{i_1} \otimes e_{i_2} \otimes \cdots \otimes e_{i_k}}$ auch \emph{Standardbasis von $V^{\otimes k}$}.

Diese Basis wird sehr häufig implizit verwendet (siehe Aufgabe \ref{ex:TensorenMitIndizes}).
\end{definition}

\begin{remark}[Index-Notation -- Was ist das?]\label{tensoren:index_notation}
Insbesondere heißt das, dass jeder Tensor $t\in V\otimes W$ eine eindeutige Koordinaten-Darstellung bzgl. solch einer Basis hat, d.h. es gibt eindeutig bestimmte Skalare $t_{ij}$ sodass
\[t = \sum_{i\in I, j\in J} t_{ij} b_i\otimes c_j\]
gilt und umgekehrt liefert jede Wahl von $\abs{I}\times \abs{J}$ vielen Skalaren $t_{ij}$ auf diese Weise genau einen Tensor.

Es hat sich in der Physik und Ingenieurwissenschaften daher leider (unhilfreicherweise\footnote{Wir nennen ja keine Namen, aber wer den Fehler macht, Johannes zu fragen, wieso er das unhilfreich findet, darf sich die nächste halbe bis dreiviertel Stunde genaustens all die vielen Gründe anhören. Wer lesen will, blättere vor bis Abschnitt \ref{tensoren:diskussion_basen}.}) eingebürgert, das Wort \enquote{Tensor} austauschbar mit \enquote{Ansammlungen von Zahlen, die mehrere Indizes haben} zu verwenden.
\end{remark}

\subsection{Lineare Abbildungen zwischen Tensoren und ihre Klassifikation}

\begin{remark}[Wozu Brauer-Diagramme?]\label{brauer:bem:klassifikation}
Sobald man Tensoren kennt und sich fest vornimmt, nicht den Fehler früherer Generationen zu wiederholen und von Anfang an basis-frei mit Tensoren zu arbeiten, dann stellt sich sofort die Frage, welche Abbildungen zwischen Tensoren natürlich genug sind, um basis-frei beschreibbar zu sein. Diese Abbildungen sind durch \enquote{Brauer-Diagramme} klassifizierbar.

Wir werden in einem folgenden Kapitel beweisen: Die Menge aller sinnvollen / natürlichen linearen Abbildungen zwischen Tensoren ist gleich Menge aller Linearkombinationen von allen Brauer-Diagrammen.
\end{remark}

\begin{definition}[Brauer\footnote{Richard Dagobert Brauer, 1901 -- 1977, dt. Mathematiker}-Diagramm und deren Multiplikation]
	Ein \emph{Brauer-Diagramm} ist ein besonderer Graph. Er besteht aus zwei Zeilen von Knoten, $n$, $n\in\IN$, Knoten in der oberen Zeile und $n+2z$, $z\in\IZ$, Knoten in der unteren Zeile. Die Knoten sind paarweise miteinander verbunden, dabei dürfen die Verbindungen sowohl von einer Zeile zur anderen gehen, als auch innerhalb einer Zeile zwei Knoten miteinander verbinden.
	
    \smallbreak
	Brauer-Diagramme verstehen wir als visuelle Darstellung von bestimmten linearen Abbildungen $V^{\otimes n} \to V^{\otimes(n+2z)}$ für einen festen $K$-Vektorraum $V$. Entsprechend können sie miteinander addiert und auch mit Skalaren aus $K$ multipliziert werden.
	
    \smallbreak
	Zwei Brauer-Diagramme können außerdem \emph{miteinander multipliziert} werden zu einem dritten Brauer-Diagramm. Dies entspricht der Hintereinanderausführung zweier linearer Abbildungen. Ausgerechnet wird das Brauer-Diagramm der zusammengesetzten Abbildung, indem die zwei Brauer-Diagramme übereinander geschrieben werden und die Knoten der unteren Zeile vom ersten Diagramm reihenfolgeerhaltend mit den Knoten der oberen Zeile vom zweiten Diagramm verbunden werden. Falls geschlossene Kreise in diesem 4-zeiligen Graph existieren, so wird das Ergebnis für jeden geschlossenen Kreis mit einem Skalarfaktor $\alpha$ multipliziert, für unsere Anwendungen ist $\alpha$ gleich der Dimension des Vektorraums $V$, von dem wir Tensorprodukte bilden,
	\[\alpha = \dim{V}.\]
	Bei $k$ geschlossenen Kreisen ist das Ergebnis dann der Graph, der die Verbindung der obersten Knoten-Zeile zur untersten Knoten-Zeile darstellt multipliziert mit $\alpha^k$.
\end{definition}

\begin{example}[Brauer-Diagramme]
	Folgende Graphen sind alles Brauer-Diagramme:
	\begin{align*}
		B_1&=\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
			\foreach\x in {1,2,...,4}{
				\node[v] (s\x) at (\x,1){};
				\node[v] (h\x) at (\x,0){};
			}
			\foreach\x in {1,2,...,4}{
				\draw (s\x) to (h\x);
			}
		\end{tikzpicture}
	& 
	B_3&=\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
		\foreach\x in {1,2,...,7}{
			\node[v] (s\x) at (\x,1){};
		}
		\foreach\x in {1,2,...,4}{
			\node[v] (h\x) at (\x,0){};
		}
		\node[v] (h6) at (6,0){};
		\draw (s1) to (h3);
		\draw (s2) to (h1);
		\draw (s6) to (h6);
		\draw (s3) to [bend right=30] (s4);
		\draw (s5) to [bend right=30] (s7);
		\draw (h2) to [bend left=30] (h4);
	\end{tikzpicture}	
	\\ \\ \\
	B_2&=\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
		\foreach\x in {1,2,...,5}{
			\node[v] (s\x) at (\x,0){};
		}
		\node[v] (h3) at (3,1){};
		\foreach\x [evaluate=\x as \y using {int(\x+1)}]in {1,4}{
			\draw (s\x) to [bend left=30] (s\y);
		}
		\foreach\x in {3}{
			\draw (s\x) to (h\x);
		}
	\end{tikzpicture}
	\hspace{1cm}
	&
	B_4&=\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
			\foreach\x in {1,2,...,5}{
				\node[v] (s\x) at (\x,1){};
			}
			\foreach\x in {2,3,4}{
				\node[v] (h\x) at (\x,0){};
			}
			\draw (s5) to (h2);
			\draw (s1) to [bend right=30] (s3);
			\draw (s2) to [bend right=30] (s4);
			\draw (h3) to [bend left=30] (h4);
		\end{tikzpicture}
	\end{align*}
	$B_1$ ist die Identität auf $V^{\otimes 4}$.
\end{example}

\begin{example}[Multiplikation zweier Brauer-Diagramme]
	Sei $d$ die Dimension des Vektorraums $V$, auf dessen Tensorpotenzen wir lineare Abbildungen betrachten. Das Produkt von $B_3$ und $B_4$ im oberen Beispiel, bzw. die lineare Abbildung, die durch $B_4 \circ B_3$ beschrieben wird, wird dann folgendermaßen berechnet:
	\begin{align*}
		\textcolor{red}{B_4} \textcolor{purple}{\circ} \textcolor{blue}{B_3}&=
        \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
			\foreach\x in {1,2,...,7}{
				\node[b] (t\x) at (\x,2.5){};
			}
			\foreach\x in {1,2,...,4}{
				\node[b] (r\x) at (\x,1.5){};
			}
			\node[b] (r6) at (6,1.5){};
			\draw[blue] (t1) to (r3);
			\draw[blue] (t2) to (r1);
			\draw[blue] (t6) to (r6);
			\draw[blue] (t3) to [bend right=30] (t4);
			\draw[blue] (t5) to [bend right=30] (t7);
			\draw[blue] (r2) to [bend left=30] (r4);
			\foreach\x in {1,2,...,5}{
				\node[r] (s\x) at (\x,1){};
			}
			\foreach\x in {2,3,4}{
				\node[r] (h\x) at (\x,0){};
			}
			\draw[red] (s5) to (h2);
			\draw[red] (s1) to [bend right=30] (s3);
			\draw[red] (s2) to [bend right=30] (s4);
			\draw[red] (h3) to [bend left=30] (h4);
			\foreach\x in {1,2,3,4}{
				\draw[purple] (r\x) to (s\x);
			}
			\draw[purple] (r6) to (s5);
		\end{tikzpicture}
	\\ \\
	&= d^1 \cdot \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
		\foreach\x in {1,2,...,7}{
			\node[v] (t\x) at (\x,1){};
		}
		\foreach\x in {3,4,5}{
			\node[v] (h\x) at (\x,0){};
		}
		\draw (t1) to [bend right=30] (t2);
		\draw (t3) to [bend right=30] (t4);
		\draw (t5) to [bend right=30] (t7);
		\draw (h4) to [bend left=30] (h5);
		\draw (t6) to (h3);
	\end{tikzpicture}
	\end{align*}
    d.h. es ist das $d$-fache der linearen Abbildung, die durch das Brauer-Diagramm in der letzten Zeile gegeben ist.
\end{example}

Auf einen reinen Tensor angewandt werden Brauer-Diagramme, indem jeder einzelne Tensorfaktor reihenfolgeerhaltend den Knoten der oberen Zeile zugeordnet wird, auf gemischte Tensoren mit linearer Fortsetzung. Dabei sind mit entsprechender Definition (siehe unten) die Brauer-Diagramme eine vollständige Beschreibung aller \emph{nützlichen} bzw. \emph{natürlichen} Arten von linearen Abbildungen. Was genau eine Abbildung nützlich macht, werden wir später mit Hilfe von Darstellungstheorie herausfinden. Zunächst wollen wir hier die verschiedenen linearen Abbildungen zwischen Tensoren klassifizieren:

\begin{example}[Permutation bilden von einem Tensor]
	\label{def:permutation}
	Sei $\sigma\in S_n$ eine Permutation. Dann definiert die Abbildung
	\[\Pi_{\sigma} : 
	\left\lbrace\begin{array}{rcl}
		V^{\otimes n} &\to& V^{\otimes}\\
		v_1 \otimes \cdots \otimes v_n &\mapsto&  v_{\sigma^{-1}(1)} \otimes \cdots \otimes  \otimes v_{\sigma^{-1}(n)}
	\end{array}\right.
	\]
	die \udot{durch $\sigma^{-1}$ bestimmte Permutation der Faktoren} des Tensors. Für einen allgemeinen Tensor ist diese Permutation als lineare Fortsetzung der obigen Abbildung gegeben.
	
    \smallbreak
	Das Brauer-Diagramm von $\Pi_\sigma$ enthält in beiden Zeilen $n$ Knoten und die Kanten verbinden nur Knoten von der oberen Zeile mit Knoten der unteren Zeile. Die genauen Verbindungen ergeben sich aus der Permutation $\sigma^{-1}$. Jedem Input $k$ von $\sigma^{-1}$ wird das Ergebnis $l=\sigma^{-1}(k)$ zugeordnet. Im Brauer-Diagramm wird dann der $k$-te Knoten der oberen Zeile (gezählt von links nach rechts) mit dem $l$-ten Knoten der unteren Zeile verbunden. Mit der Zwei-Zeilen-Schreibweise von $\sigma^{-1}$ lässt sich dies gut ablesen, z.B. sei:
	\begin{align*}
		\sigma &= \begin{bmatrix}1 & 2 & 3 & 4 & 5 \\ 4 & 3 & 1 & 5 & 2 \end{bmatrix},
		\\
		\implies\sigma^{-1} &= \begin{bmatrix}4 & 3 & 1 & 5 & 2 \\ 1 & 2 & 3 & 4 & 5\end{bmatrix} = \begin{bmatrix}1 & 2 & 3 & 4 & 5 \\ 3 & 5 & 2 & 1 & 4 \end{bmatrix}.\\
	\end{align*}
	Das zu $\Pi_{\sigma}$ zugehörige Brauer-Diagramm ist dann:
	\begin{align*}
		\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
			\foreach\x in {1,2,...,5}{
				\node[v] (s\x) at (\x,1){};
				\node[v] (h\x) at (\x,0){};
			}
			\draw (s1) to (h3);
			\draw (s2) to (h5);
			\draw (s3) to (h2);
			\draw (s4) to (h1);
			\draw (s5) to (h4);		
		\end{tikzpicture}.
	\end{align*}

    Insbesondere ist $\Pi_{\id}$ die identische Abbildung $V^{\otimes n} \to V^{\otimes n}$.
\end{example}

\begin{example}[Spur nehmen von einem Tensor]
	\label{def:spur}
Es sei auf $V$ ein Skalarprodukt $\braket{\cdot,\cdot}$ gegeben. Dann definiert die Abbildung
\[\tr_{kl} : 
\left\lbrace\begin{array}{rcl}
	V^{\otimes n} &\to& V^{\otimes n-2}\\
	v_1 \otimes \cdots \otimes v_k \otimes \cdots \otimes v_l \otimes  \cdots \otimes v_n &\mapsto& \braket{v_k,v_l} v_1 \otimes \cdots \otimes \hat{v_k} \otimes \cdots \otimes \hat{v_l} \otimes  \cdots \otimes v_n
\end{array}\right.
\]
die \udot{Spur zwischen dem $k$-ten und $l$-ten Faktor} des Tensors. Für einen allgemeinen Tensor ist die Spur als lineare Fortsetzung der obigen Abbildung gegeben.

\smallbreak
Das Brauer-Diagramm der Spur besteht aus $n$ Knoten in der oberen und $n-2$ Knoten in der unteren Zeile. Der $k$-te und $l$-te Knoten der oberen Zeile sind mit einer Kante verbunden, solche Kanten nennen wir \emph{waagerecht}. Alle anderen Knoten sind mit ausschließlich senkrechten Kanten verbunden, z.B. für $\tr_{6,7}$ auf $V^{\otimes 7}$:
	\begin{align*}
	\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
		\foreach\x in {1,2,...,7}{
			\node[v] (s\x) at (\x,1){};
		}
		\foreach\x in {1,2,...,5}{
			\node[v] (h\x) at (\x,0){};
		}
		\foreach\x in {1,2,...,5}{
			\draw (s\x) to (h\x);
		}
		\draw (s6) to [bend right=30] (s7);
	\end{tikzpicture}
	\end{align*}
Diese Definition ist auch in entgegengesetzte Richtung zu verstehen: Wenden wir ein Brauer-Diagramm, welches zwei obere Knoten miteinander verbindet, auf einen reinen Tensor an, so ist zwischen den beiden entsprechenden Faktoren des Tensors die Spur zu bilden.

(In den Aufgaben findet sich auch eine, die erklärt, wieso diese Abbildung ausgerechnet \enquote{Spur} heißt)
\end{example}

\begin{example}[Einfügen des Casimir-Elements]
	\label{def:casimireinfuegen}
	Sei $\Omega_V$ das Casimir-Element zu $V$. Dann definiert die Abbildung
	\[\casimir : 
	\left\lbrace\begin{array}{rcl}
		V^{\otimes n} &\to& V^{\otimes n+2}\\
		v_1 \otimes \cdots \otimes v_n &\mapsto& v_1 \otimes \cdots \otimes  v_n \otimes \Omega_V
	\end{array}\right.
	\]
	die \udot{Einfügung des Casimir-Elements} an den Tensor. Für einen allgemeinen Tensor ist die Einfügen des Casimir-Elements als lineare Fortsetzung der obigen Abbildung gegeben.
	
    \smallbreak
	Das Brauer-Diagramm der Einfügung des Casimir-Elements besteht aus $n$ Knoten in der oberen und $n+2$ Knoten in der unteren Zeile. Die letzten beiden Knoten der unteren Zeile sind mit einer Kante verbunden. Alle anderen Knoten sind mit ausschließlich senkrechten Kanten verbunden, z.B. :
	\begin{align*}
		\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
			\foreach\x in {1,2,...,7}{
				\node[v] (s\x) at (\x,0){};
			}
			\foreach\x in {1,2,...,5}{
				\node[v] (h\x) at (\x,1){};
			}
			\foreach\x in {1,2,...,5}{
				\draw (s\x) to (h\x);
			}
			\draw (s6) to [bend left=30] (s7);
		\end{tikzpicture}
	\end{align*}
	Diese Definition ist auch in entgegengesetzte Richtung zu verstehen: Wenden wir ein Brauer-Diagramm, welches zwei untere Knoten miteinander verbindet, auf einen reinen Tensor an, so ist an den beiden entsprechenden Faktoren des Tensors das Casimir-Element einzufügen.
\end{example}

\begin{remark}
	An welchen zwei Stellen die zwei Faktoren des Casimir-Elements eingefügt werden, kann man sich im Prinzip aussuchen, das entspricht dann der Hintereinanderausführung von $\Omega_V$-Einfügung und Permutation. Insbesondere können die beiden Hälften von $\Omega_V$ auch voneinander getrennt werden, z.B. wird mit
	\[
	\sigma=\begin{bmatrix}1 & 2 & \cdots & n+1 & n+2 \\n+2 & 2 & \cdots & n+1 & 1 \end{bmatrix}\in S_{n+2}
	\]
	durch 
	\begin{equation}
		\label{eq:sigmaOfCasimir}
	\Pi_{\sigma} \circ \casimir (v_1 \cdots \otimes \cdots v_n)
	\end{equation}
	das mit $\casimir$ eingefügte $\Omega_V$ in zwei Teile zerlegt.
\end{remark}

\begin{remark}
Man überlegt sich jetzt leicht, dass jedes Brauer-Diagramm als Multiplikation von (mehreren) Permutationen, Spuren und Casimir-Elementen erhalten werden kann, diese drei Beispiele also im Wesentlichen alle Fälle abdecken.
\end{remark}

\begin{remark}[Bilineare Abbildungen]
Bilineare, tri- und $k$-fach lineare Abbildungen zwischen Tensoren lassen sich auf lineare Abbildungen zurückführen, indem man das Tensorprodukt der Input-Tensoren bildet und dann eine beliebige lineare Abbildung ausführt. Unsere angestrebte Klassifikation behandelt also in Wirklichkeit mehr als nur den linearen Fall.
\end{remark}

\subsection{Anhang: Sinn und Unsinn von Basen}\label{tensoren:diskussion_basen}

\begin{remark}[Ko- und kontravariantes Transformieren]
Wir haben in \ref{tensoren:index_notation} bereits beschrieben, wie man mittels einer Basis von $V$ jedem Tensor aus $V^{\otimes k}$ seine Koordinaten bzgl. dieser Basis zuordnen kann.

Wählt man eine andere Basis von $V$, so erhält man natürlich andere Zahlen als Koordinaten. Immer noch $\dim(V)^k$ viele, aber andere. Jedoch sind die Unterschiede i.A. nicht völlig beliebig (wie Fall $k=1$), denn sie sind eben durch einen Basiswechsel in den Faktoren des Tensorprodukts verursacht.

In der Tat haben die Physiker eigene Sprechweise erfunden, um das auszudrücken: Man sagt, dass sich ein Satz von indizierten Zahlen \enquote{kovariant transformiert} (bzw. \enquote{kontravariant}) und meint damit nicht etwa, dass diese Zahlen eine besondere Eigenschaft hätten, sondern, dass die Methode, mit der man diese Zahlen bestimmt hat, eine besondere Eigenschaft hat, nämlich bei anderer Basiswahl ein anderes Ergebnis zu produzieren, dass aber aus der Basiswechselmatrix auf bestimmte Weise vorhersehbar ist. Manche Physiker nehmen das zum Anlass, um zu definieren, dass ein Tensor nicht nur \emph{ein} Satz von indizierten Zahlen sei, sondern eine Abbildung $\Set{\text{Basen}} \to \Set{\text{Zahlen mit Indizes}}$, die genau dieser Transformationsregel gehorcht.

\medbreak
Soweit, so verwirrend, aber natürlich wird einem dann auch erklärt, dass es auch \enquote{kontravariante Tensoren} gibt, die sich irgendwie ähnlich, aber ganz anders verhalten, sowie gemischte Tensoren, bei denen manche Indizes ko- und manche kontravariant gemeint sind. In Indexschreibweise äußert sich das oft so, dass manche Indizes oben und manche unten notiert werden, wobei sich leider nicht einmal alle Physiker einig sind, ob die oberen jetzt die ko- oder die kontravarianten sein sollen.
\end{remark}

\begin{remark}[Index-Notation -- Warum ist das ein Problem?]
Index-Notation erschwert durch die Fixierung auf Basen und Koordinaten im Wesentlichen alles, was wichtig ist: Das Verstehen von und das Arbeiten mit Tensoren.

\begin{itemize}
\item Index-Schreibweise -- Koordinaten allgemein -- verlieren wichtige Informationen und geht am Wesentlichen vorbei: Ein Vektor/Tensor ist eben nicht eine Ansammlung von Zahlen, sondern ein Element eines Vektorraums.

Ja, der Raum der Polynome in zwei Variablen mit Grad $\leq 5$ ist 36-dimensional, aber das Polynom $x^2+y^2$ deshalb als Ansammlung von 36 (relativ beliebigen) Zahlen zu betrachten, also als Element von $\IR^{36}$, verliert die ganz wesentliche Information, z.B. dass es sich um ein Polynom handelt, dass dieser Vektorraum Unterraum eines größeren Polynomraums ist, in dem weitere Strukturen wie z.B. Polynommultiplikation, Differenzieren, uvm. existieren.

Der Übergang von abstrakten Vektoren/Tensoren zu Koordinatendarstellungen bzgl. beliebiger Basen, erhält exakt die Information, die in der Vektorraumstruktur alleine steckt, d.h. in Addition und Skalarmultiplikation. Alle zusätzliche Struktur, die in einer konkreten Situation vielleicht außerdem vorhanden und nützlich (!) sein könnte, wird weggeworfen.

Wenn man alles nur als Ansammlung von Zahlen versteht, ist auch klar, wieso der Unterschied zwischen \enquote{ko-} und \enquote{kontravarianten} Tensoren so mysteriös ist. Wenn man abstrakt arbeitet, ist ganz klar, was passiert! Das sind einfach Tensoren aus unterschiedlichen Räumen!

Die eine Sorte Tensoren ist Element von $V^{\otimes k}$, während die anderen Elemente von $(V^\ast)^{\otimes k}$ sind. Tensoren mit gemischter Varianz sind entsprechend Elemente von $V^{\otimes k_1}\otimes (V^\ast)^{\otimes k_2}$. Natürlich sind $V$ und $V^\ast$ als Vektorräume isomorph, d.h. sie haben die gleiche Dimension, man benötigt also die gleiche Anzahl an Indizes. Aber dabei verliert man eben die wesentliche Information, dass $V$ und $V^\ast$ doch trotz allen verschieden sind und verschiedene Objekte und Sichtweisen mathematisch beschreiben.\footnote{Übrigens erklärt diese Sichtweise auch, wieso sich die Physiker nicht immer einig sind, was ko- und was kontravariant ist: Für endlich-dimensionale Vektorräume ist nicht nur $V^\ast$ der duale Vektorraum von $V$, sondern auch $V$ der duale Vektorraum von $V^\ast$. Man kann sich also aussuchen, welches das fundamentalere Objekt von beiden ist.}

Für uns wird z.B. ganz wesentlich sein, dass Tensorräume Zusatzstrukturen besitzen, die man in beliebigen Koordinaten nicht (oder nur mit unnötig viel Aufwand) sehen kann.

\item Index-Schreibweise -- Koordinaten allgemein -- zäumen das Pferd von hinten auf:

Wenn man einmal einen globalen Standpunkt einnimmt, erkennt man, dass es selten sinnvoll ist, tatsächlich in Koordinaten zu arbeiten, da die einzigen Operationen, die überhaupt physikalisch sinnvoll sind, zwangsläufig basis-unabhängig sein müssen. Das Universum hat halt keine bevorzugte Basis. Es gibt keine bevorzugten Richtungen im Universum; wieso also nicht von Anfang an mit Objekte und Operationen arbeiten, die basis-unabhängig sind?

Und in den Fällen, in denen die physikalische Situation eben doch eine Raumrichtung bevorzugt, z.B. weil es sich um ein rotationssymmetrisches Problem handelt und die Drehachse eine besondere Richtung in so einem System ist? In den Fällen ist es auch nicht sinnvoll, \emph{beliebige} Basen zu betrachten. Man will eine Basis wählen, die Zusatzinformationen wie z.B. die Drehsymmetrie, möglichst gut widerspiegeln.

Woher weiß man, welche Zusatzstruktur tatsächlich die nützliche ist für die eigene Frage? Man weiß es eben nicht. Zumindest nicht vorher. Erst, wenn man tatsächlich eine (Teil-)Antwort auf die Frage gefunden hat, kann man überhaupt sagen, welche Zusatzstrukturen nützlich war und welche nicht, und welche Basiswahl also die sinnvollste für das Problem ist.

Der Ausgangspunkt sollte also immer zunächst eine basis-freie Sichtweise sein, in die erst nachdem tatsächlich Bedarf erkannt wurde, eine präzise ausgewählte Basis eingeführt wird. Und selbst dann ist es sinnvoll, nicht allzu viel Basis zu wählen. Im Falle einer Rotationssymmetrie ist z.B. \emph{nur} die Drehachse eine ausgezeichnete Raumrichtung, d.h. sie legt nur einen von drei gesuchten Basisvektoren (halbwegs) fest. Für die anderen beiden gilt genau das gleiche: Man sollte solange wie möglich abstrakt arbeiten und erst, wenn das Problem es tatsächlich erfordert, eine Basis wählen.

\item Index-Schreibweise erzeugt unnötige Arbeit:

Eine beliebige Kollektion von Zahlen wird aber nur dann zu einem Tensor, wenn man eine Basis wählt, bzgl. derer man diese Zahlen dann als Koordinaten liest. Man muss sich also stets fragen: \enquote{Wenn ich eine andere Basis wähle und in dieser neuen Situation noch einmal neue Zahlen generiere und als Koordinaten bzgl. der neuen Basis benutze, kommt ein anderer Tensor heraus oder nicht?} und man will immer, dass die Antwort \enquote{Nein} ist, d.h. man will, dass sich die Zahlen \enquote{ko-} oder \enquote{kontravariant} oder \enquote{gemischt} \enquote{transformieren}. Ein \enquote{ja} hieße zwangsläufig, dass man etwas falsch gemacht hat, eben weil physikalisch sinnvolle Ergebnisse nicht basis-abhängig sind.

Die Konsequenz daraus ist aber, dass man eigentlich jedes Index-Monster, das einem begegnet, erst einmal mit viel langweiligem Rechenaufwand darauf überprüfen sollte, ob es überhaupt basis-unabhängig ist. Wer hat die Zeit dafür? Niemand! Und wieso sollte man auch so viel Aufwand investieren, wenn die Antwort, die man haben will, sowieso immer die gleiche ist?

Nicht nur macht die abstrakte Sichtweise den Aufwand kleiner, sie macht ihn zu Null, denn, wenn man von Anfang an basisfrei arbeitet, stellt sich die Frage, ob eine gewisse Konstruktion basisunabhängig ist, überhaupt nicht. Wenn nirgendwo Basen benutzt wurden, muss alles, was getan wurde, natürlich basisunabhängig sein!

\item \enquote{Aber, wenn ich Dinge ausrechnen will, brauche ich doch Basen!}

Auch das ist ein weitverbreitetes Missverständnis. Viele Dinge sind in der Tat auch völlig basisfrei berechenbar, mindestens z.B. alle physikalisch sinnvollen Dinge, denn die sind eben -- wie wir bereits festgestellt haben --  von sich aus basisfrei und somit zwangsläufig auch basisfrei berechenbar, wenn sie überhaupt berechenbar sind. Und in der Tat kann man sie dann auch immer auf einem basisfreien Wege ausrechnen.

Man muss sich auch einmal klarmachen, was man mit \enquote{ausrechnen} meint und wozu man das tut. Was bedeutet es z.B. einen Spannungstensor \enquote{auszurechnen} ? Der Spannungstensor ist eben keine Zahl. Er ist ein Tensor. Was würde es in Analogie bedeuten, einen Vektor, z.B. eine Drehachse \enquote{auszurechnen} ? Die Drehachse ist die Drehachse, das, worum sich das System dreht. Welche andere Information möchte man über diesen Vektor ausrechnen? Und was würde man damit anfangen?

Natürlich kann man zwanghaft Koordinaten einführen und die Koordinaten dieses Vektors ausrechnen. Die Koordinaten haben aber keine eigenständige physikalische Bedeutung. Mit den Koordinaten an sich lässt sich also nichts anfangen. Einzig das Gesamtobjekt ist physikalisch sinnvoll. Das einzige, was man mit den Koordinaten tun könnte, ist also, sie wieder zu einem Vektor zusammenzusetzen, um diesen in einem nächsten Rechenschritt weiterzuverwenden. Wieso dann aber Koordinaten einführen?\footnote{Das ignoriert natürlich ein paar außer-physikalische Aspekte, z.B. könnten ja in einer komplexen Gesamtberechnung einzelne Teilschritte durchaus von verschiedenen Leuten ausgerechnet werden. In diesem Fall ist das Einführen von Koordinaten eine Notwendigkeit, um ein Zwischenergebnis von einem Team zum anderen kommunizieren zu können.}

\medbreak
Man beachte, dass die Frage nach einem \emph{effizienten} Ausrechnen eine völlig andere ist! Für diese Art von Fragen gilt: \enquote{premature optimization is the root of all evil (or at least most of it)}\footnote{Donald Ervin Knuth, geb. 1938, amer. Informatiker und Mathematiker}. Eine Basiswahl alleine macht nichts effizient. Die \emph{richtige} Basis in der richtigen Situation macht Dinge effizient und auch nur, wenn man sie richtig einsetzt. Stumpf drauf losrechnen ist selten tatsächlich effizient.
\end{itemize}
\end{remark}